{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-321: Data Mining and Machine Learning\n",
    "## Assignment 5: Classification\n",
    "\n",
    "### Part 1: Cross-validation for training and testing\n",
    "\n",
    "So far we've used the same data for training and testing. That's definitely NOT a great idea. We'll now implement k-fold cross-validation. We'll pass k as a parameter, so we can decide how many folds we want to make. The general idea is that we need to split the data into subsections for training and for testing. Refer to your notes to remind yourself how cross-validation works.\n",
    "\n",
    "(a) Create a function called cross_validation_data(dataset, folds). This function is going to split our data in k folds, where k is the parameter given. The function should create (and ultimately return) a new list. We need to take a shallow copy of the data set, and operate on it. We need to determine how much data will be in each fold, by taking the length of our dataset, and dividing it by the number of folds, probably using integer division. \n",
    "\n",
    "Then we need to loop number of folds times, creating a new fold and populating that fold with data from our copy of the dataset. Roughly speaking that means:\n",
    "\n",
    "- while the amount of data in the current fold is less than the number we determined above for how much data SHOULD be in each fold\n",
    "    - choose a random instance from our copy of the data set\n",
    "        - HINT: You can use functions from the random library to help you (https://docs.python.org/3/library/random.html)\n",
    "    - Place the chosen instance into our current fold\n",
    "    - REMOVE the instance from the copy of the data\n",
    "    \n",
    "    - append the new fold to our list for returning\n",
    "- Continue until we've populated all the folds\n",
    "\n",
    "Before using whatever random method you choose to select values, we'll set the seed to 1. For this assignment it will mean that your results should be reproducably the same every time you run it, which can help with the testing. I've done that for you below.\n",
    "\n",
    "As an example, if we have a data set with 8 instances, and we split it into 4 folds, we'll have four sublists, each with two instances. E.g.\n",
    "if our input dataset was [[1,1], [2,1], [4,2], [6,1], [7,3], [8,4], [9,6], [5,2]]\n",
    "\n",
    "our output COULD be:\n",
    "\n",
    "[[[2,1], [4,2]], [[5,2], [8,4]], [[6,1],[9,6]], [[7,3], [1,1]]]\n",
    "\n",
    "NOTE the additional level of nesting, with respect to what happens in the function we write next. Create yourself a contrived test set, and try the function out. REMEMBER, that an instance can only appear in a single fold. \n",
    "\n",
    "This is a naive way to do this, and the resulting folds will definitely not be stratified, but it's better than anything we've done so far.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 folds [[[4, 2], [8, 4]], [[1, 1], [7, 3]], [[2, 1], [9, 6]], [[5, 2], [6, 1]]]\n",
      "3 folds [[[9, 6], [5, 2]], [[2, 1], [1, 1]], [[8, 4], [4, 2], [7, 3], [6, 1]]]\n",
      "2 folds [[[1, 1], [9, 6], [7, 3], [6, 1]], [[4, 2], [5, 2], [2, 1], [8, 4]]]\n",
      "1 folds [[[1, 1], [2, 1], [4, 2], [5, 2], [6, 1], [8, 4], [7, 3], [9, 6]]]\n",
      "6 folds [[[1, 1]], [[8, 4]], [[4, 2]], [[9, 6]], [[5, 2]], [[7, 3], [2, 1], [6, 1]]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#from random import seed\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "some_data = [[1,1], [2,1], [4,2], [6,1], [7,3], [8,4], [9,6], [5,2]]\n",
    "def cross_validation_data(dataset, folds):\n",
    "    dataCopy = dataset[:]\n",
    "    foldLen = len(dataCopy)//folds\n",
    "    crossData = []\n",
    "    for i in range(folds - 1):\n",
    "        currFold = []\n",
    "        for j in range(foldLen):\n",
    "            currData = random.choice(dataCopy)\n",
    "            currFold.append(currData)\n",
    "            dataCopy.pop(dataCopy.index(currData))\n",
    "        crossData.append(currFold)\n",
    "    currFold = []\n",
    "    for i in range(len(dataCopy)):\n",
    "            currData = random.choice(dataCopy)\n",
    "            currFold.append(currData)\n",
    "            dataCopy.pop(dataCopy.index(currData))\n",
    "    crossData.append(currFold)\n",
    "    return crossData\n",
    "\n",
    "cross_test = cross_validation_data(some_data, 4)\n",
    "print('4 folds', cross_test)\n",
    "cross_test = cross_validation_data(some_data, 3)\n",
    "print('3 folds', cross_test)\n",
    "cross_test = cross_validation_data(some_data, 2)\n",
    "print('2 folds', cross_test)\n",
    "cross_test = cross_validation_data(some_data, 1)\n",
    "print('1 folds', cross_test)\n",
    "cross_test = cross_validation_data(some_data, 6)\n",
    "print('6 folds', cross_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Now you need to amend the evaluate algorithm function I've given you previously. I've changed the signature to include a folds parameter. This function needs to:\n",
    "\n",
    "- Use your function from (a) above, to create k folds of data\n",
    "- Create an empty list of scores\n",
    "- For each fold in your evaluation\n",
    "    - (1) set up the training set for that fold\n",
    "    - (2) You can think of that as REMOVING the fold which will be used for evaluation in this fold from the data you returned from (a)\n",
    "    - (3) BE CAREFUL to always operate on copies of the data - you don't want to mess up your original splits\n",
    "    - (4) You then need to 'flatten' the remaining data in the current training set\n",
    "        - (5) For example if the data was as given above:\n",
    "            - (6) [[[2,1], [4,2]], [[5,2], [8,4]], [[6,1],[9,6]], [[7,3], [1,1]]]\n",
    "        - (7) We would remove ONE fold first for testing (let's say the last one, but it will be each fold in turn)\n",
    "        - (8) Leaving [[[2,1], [4,2]], [[5,2], [8,4]], [[6,1],[9,6]]] for training\n",
    "        - (9) We need to convert that into: [[2,1], [4,2], [5,2], [8,4], [6,1],[9,6]]\n",
    "        - (10) This is the usual format we pass data into our algorithms\n",
    "    - (11) We also need to prepare our test set, which is the held-out fold from step (2) above\n",
    "    - (12) Append the instances from this held-out fold to a new list making sure the last value of each instance is NONE\n",
    "    - Once this is done, you should have a training set, comprised on k-1 folds of instances, and a test set, comprised of 1 fold of instances. We're now ready to evaluate our algorithm just as we have previously, using a training set, a test set and a specified evaluation metric. This should return a score to us. Instead of using that score directly, we should add it to our score list\n",
    "- At the end of this function, RETURN the complete list of scores. Therefore, if we did a 5 fold cross validation, we should get a list of 5 scores.\n",
    "\n",
    "I've given you the (very) bare bones of the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_algorithm(dataset, algorithm, folds, metric, *args):\n",
    "    foldedData = cross_validation_data(dataset, folds)\n",
    "    scores = []\n",
    "    for i in range(len(foldedData)):\n",
    "        copyFolded = foldedData[:]\n",
    "        test_data = copyFolded.pop(i)\n",
    "        test = [test_data[j][:-1] for j in range(len(test_data))]\n",
    "        for j in test:\n",
    "            j.append(None)\n",
    "        train = []\n",
    "        for fold in copyFolded:\n",
    "            train += fold\n",
    "        predicted = algorithm(train,test, *args)\n",
    "        actual = [j[-1] for j in test_data]\n",
    "        result = metric(actual,predicted)\n",
    "        scores.append(result)\n",
    "\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Applying cross-validation to real data\n",
    "\n",
    "To test the function you wrote above, let's apply it to the multivariate linear regression you wrote last week. Copy the function you wrote above to the cell below, along with all the code you need for BOTH MLR and zeroRR to work.\n",
    "Use the same parameters I gave you last week (a learning rate of 0.01 and 50 epochs of training), run MLR using a cross-validation of 5 folds. PRINT out the list of RMSE scores obtained on each fold. Then run zeroRR. \n",
    "Also print the LOWEST score obtained (that's the best), the highest score (that's the worst) and the mean RMSE score. See for yourself the variance in scores you can obtain using a cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 lrate= 0.01 error= 77.40535070937612\n",
      "epoch= 1 lrate= 0.01 error= 66.12006675767817\n",
      "epoch= 2 lrate= 0.01 error= 64.49568515584596\n",
      "epoch= 3 lrate= 0.01 error= 63.605839814163886\n",
      "epoch= 4 lrate= 0.01 error= 63.08541932553708\n",
      "epoch= 5 lrate= 0.01 error= 62.76957015389195\n",
      "epoch= 6 lrate= 0.01 error= 62.57100159044884\n",
      "epoch= 7 lrate= 0.01 error= 62.441976420747345\n",
      "epoch= 8 lrate= 0.01 error= 62.355529458815695\n",
      "epoch= 9 lrate= 0.01 error= 62.295906991926906\n",
      "epoch= 10 lrate= 0.01 error= 62.25360416693796\n",
      "epoch= 11 lrate= 0.01 error= 62.222717576458344\n",
      "epoch= 12 lrate= 0.01 error= 62.199487759025615\n",
      "epoch= 13 lrate= 0.01 error= 62.18146991672429\n",
      "epoch= 14 lrate= 0.01 error= 62.16704654765324\n",
      "epoch= 15 lrate= 0.01 error= 62.15513224788399\n",
      "epoch= 16 lrate= 0.01 error= 62.144990048992405\n",
      "epoch= 17 lrate= 0.01 error= 62.13611449969259\n",
      "epoch= 18 lrate= 0.01 error= 62.12815580912337\n",
      "epoch= 19 lrate= 0.01 error= 62.12086986655685\n",
      "epoch= 20 lrate= 0.01 error= 62.114084900315476\n",
      "epoch= 21 lrate= 0.01 error= 62.107679013970625\n",
      "epoch= 22 lrate= 0.01 error= 62.101564927963466\n",
      "epoch= 23 lrate= 0.01 error= 62.09567954496693\n",
      "epoch= 24 lrate= 0.01 error= 62.08997677192312\n",
      "epoch= 25 lrate= 0.01 error= 62.0844225558403\n",
      "epoch= 26 lrate= 0.01 error= 62.07899143296937\n",
      "epoch= 27 lrate= 0.01 error= 62.07366411760111\n",
      "epoch= 28 lrate= 0.01 error= 62.06842580815842\n",
      "epoch= 29 lrate= 0.01 error= 62.063264990231346\n",
      "epoch= 30 lrate= 0.01 error= 62.05817258529981\n",
      "epoch= 31 lrate= 0.01 error= 62.05314134094905\n",
      "epoch= 32 lrate= 0.01 error= 62.04816539055146\n",
      "epoch= 33 lrate= 0.01 error= 62.04323993247181\n",
      "epoch= 34 lrate= 0.01 error= 62.03836099404154\n",
      "epoch= 35 lrate= 0.01 error= 62.03352525603421\n",
      "epoch= 36 lrate= 0.01 error= 62.028729920635755\n",
      "epoch= 37 lrate= 0.01 error= 62.023972610927515\n",
      "epoch= 38 lrate= 0.01 error= 62.01925129341377\n",
      "epoch= 39 lrate= 0.01 error= 62.01456421756149\n",
      "epoch= 40 lrate= 0.01 error= 62.00990986803414\n",
      "epoch= 41 lrate= 0.01 error= 62.00528692651232\n",
      "epoch= 42 lrate= 0.01 error= 62.00069424083014\n",
      "epoch= 43 lrate= 0.01 error= 61.99613079976953\n",
      "epoch= 44 lrate= 0.01 error= 61.99159571228537\n",
      "epoch= 45 lrate= 0.01 error= 61.98708819023104\n",
      "epoch= 46 lrate= 0.01 error= 61.98260753389719\n",
      "epoch= 47 lrate= 0.01 error= 61.978153119819446\n",
      "epoch= 48 lrate= 0.01 error= 61.97372439044652\n",
      "epoch= 49 lrate= 0.01 error= 61.96932084534196\n",
      "epoch= 0 lrate= 0.01 error= 76.20889867232674\n",
      "epoch= 1 lrate= 0.01 error= 64.95458531372677\n",
      "epoch= 2 lrate= 0.01 error= 63.43926307053889\n",
      "epoch= 3 lrate= 0.01 error= 62.62030529531734\n",
      "epoch= 4 lrate= 0.01 error= 62.1389401791107\n",
      "epoch= 5 lrate= 0.01 error= 61.845075377301086\n",
      "epoch= 6 lrate= 0.01 error= 61.659485508883066\n",
      "epoch= 7 lrate= 0.01 error= 61.53836554264812\n",
      "epoch= 8 lrate= 0.01 error= 61.456721424224774\n",
      "epoch= 9 lrate= 0.01 error= 61.399878539909494\n",
      "epoch= 10 lrate= 0.01 error= 61.35898915611478\n",
      "epoch= 11 lrate= 0.01 error= 61.32858709113599\n",
      "epoch= 12 lrate= 0.01 error= 61.30522006989585\n",
      "epoch= 13 lrate= 0.01 error= 61.286664292169725\n",
      "epoch= 14 lrate= 0.01 error= 61.27146173472286\n",
      "epoch= 15 lrate= 0.01 error= 61.2586409975685\n",
      "epoch= 16 lrate= 0.01 error= 61.24754518379669\n",
      "epoch= 17 lrate= 0.01 error= 61.237723700165546\n",
      "epoch= 18 lrate= 0.01 error= 61.228863079277545\n",
      "epoch= 19 lrate= 0.01 error= 61.22074209773443\n",
      "epoch= 20 lrate= 0.01 error= 61.213202285966005\n",
      "epoch= 21 lrate= 0.01 error= 61.20612833581795\n",
      "epoch= 22 lrate= 0.01 error= 61.199434955043515\n",
      "epoch= 23 lrate= 0.01 error= 61.19305796724995\n",
      "epoch= 24 lrate= 0.01 error= 61.18694823408544\n",
      "epoch= 25 lrate= 0.01 error= 61.18106746913377\n",
      "epoch= 26 lrate= 0.01 error= 61.17538532923874\n",
      "epoch= 27 lrate= 0.01 error= 61.16987737443256\n",
      "epoch= 28 lrate= 0.01 error= 61.16452362244235\n",
      "epoch= 29 lrate= 0.01 error= 61.1593075129542\n",
      "epoch= 30 lrate= 0.01 error= 61.154215156274454\n",
      "epoch= 31 lrate= 0.01 error= 61.149234780888364\n",
      "epoch= 32 lrate= 0.01 error= 61.144356321315115\n",
      "epoch= 33 lrate= 0.01 error= 61.13957110585358\n",
      "epoch= 34 lrate= 0.01 error= 61.13487161621961\n",
      "epoch= 35 lrate= 0.01 error= 61.13025129953564\n",
      "epoch= 36 lrate= 0.01 error= 61.125704418947116\n",
      "epoch= 37 lrate= 0.01 error= 61.12122593314692\n",
      "epoch= 38 lrate= 0.01 error= 61.11681139785869\n",
      "epoch= 39 lrate= 0.01 error= 61.11245688426606\n",
      "epoch= 40 lrate= 0.01 error= 61.10815891072135\n",
      "epoch= 41 lrate= 0.01 error= 61.10391438502239\n",
      "epoch= 42 lrate= 0.01 error= 61.099720555224714\n",
      "epoch= 43 lrate= 0.01 error= 61.09557496743924\n",
      "epoch= 44 lrate= 0.01 error= 61.09147542941677\n",
      "epoch= 45 lrate= 0.01 error= 61.0874199789791\n",
      "epoch= 46 lrate= 0.01 error= 61.08340685655189\n",
      "epoch= 47 lrate= 0.01 error= 61.07943448118476\n",
      "epoch= 48 lrate= 0.01 error= 61.0755014295705\n",
      "epoch= 49 lrate= 0.01 error= 61.071606417648496\n",
      "epoch= 0 lrate= 0.01 error= 78.74278001060844\n",
      "epoch= 1 lrate= 0.01 error= 67.23209025811025\n",
      "epoch= 2 lrate= 0.01 error= 65.48970927797431\n",
      "epoch= 3 lrate= 0.01 error= 64.55001158635055\n",
      "epoch= 4 lrate= 0.01 error= 64.00375660280058\n",
      "epoch= 5 lrate= 0.01 error= 63.672561800142674\n",
      "epoch= 6 lrate= 0.01 error= 63.46410541737401\n",
      "epoch= 7 lrate= 0.01 error= 63.328373775136576\n",
      "epoch= 8 lrate= 0.01 error= 63.23720519649555\n",
      "epoch= 9 lrate= 0.01 error= 63.17414933050837\n",
      "epoch= 10 lrate= 0.01 error= 63.12926848451185\n",
      "epoch= 11 lrate= 0.01 error= 63.09637999932815\n",
      "epoch= 12 lrate= 0.01 error= 63.07154069716184\n",
      "epoch= 13 lrate= 0.01 error= 63.052183842362595\n",
      "epoch= 14 lrate= 0.01 error= 63.03661072904065\n",
      "epoch= 15 lrate= 0.01 error= 63.02368181305496\n",
      "epoch= 16 lrate= 0.01 error= 63.012623991673635\n",
      "epoch= 17 lrate= 0.01 error= 63.002907641668344\n",
      "epoch= 18 lrate= 0.01 error= 62.99416672995361\n",
      "epoch= 19 lrate= 0.01 error= 62.986146150457806\n",
      "epoch= 20 lrate= 0.01 error= 62.978666605173615\n",
      "epoch= 21 lrate= 0.01 error= 62.97160096655695\n",
      "epoch= 22 lrate= 0.01 error= 62.96485824642088\n",
      "epoch= 23 lrate= 0.01 error= 62.958372653512015\n",
      "epoch= 24 lrate= 0.01 error= 62.95209608202004\n",
      "epoch= 25 lrate= 0.01 error= 62.945992928096004\n",
      "epoch= 26 lrate= 0.01 error= 62.940036494481646\n",
      "epoch= 27 lrate= 0.01 error= 62.934206483614474\n",
      "epoch= 28 lrate= 0.01 error= 62.92848723996627\n",
      "epoch= 29 lrate= 0.01 error= 62.92286651023975\n",
      "epoch= 30 lrate= 0.01 error= 62.917334562999955\n",
      "epoch= 31 lrate= 0.01 error= 62.91188355886535\n",
      "epoch= 32 lrate= 0.01 error= 62.90650709618713\n",
      "epoch= 33 lrate= 0.01 error= 62.90119988028295\n",
      "epoch= 34 lrate= 0.01 error= 62.89595748016308\n",
      "epoch= 35 lrate= 0.01 error= 62.890776147621786\n",
      "epoch= 36 lrate= 0.01 error= 62.88565268111019\n",
      "epoch= 37 lrate= 0.01 error= 62.88058432203357\n",
      "epoch= 38 lrate= 0.01 error= 62.87556867473928\n",
      "epoch= 39 lrate= 0.01 error= 62.870603643991345\n",
      "epoch= 40 lrate= 0.01 error= 62.865687385495626\n",
      "epoch= 41 lrate= 0.01 error= 62.86081826627014\n",
      "epoch= 42 lrate= 0.01 error= 62.855994832545846\n",
      "epoch= 43 lrate= 0.01 error= 62.85121578348532\n",
      "epoch= 44 lrate= 0.01 error= 62.84647994946111\n",
      "epoch= 45 lrate= 0.01 error= 62.84178627394868\n",
      "epoch= 46 lrate= 0.01 error= 62.83713379831547\n",
      "epoch= 47 lrate= 0.01 error= 62.83252164895986\n",
      "epoch= 48 lrate= 0.01 error= 62.827949026372615\n",
      "epoch= 49 lrate= 0.01 error= 62.823415195790545\n",
      "epoch= 0 lrate= 0.01 error= 79.55922171097885\n",
      "epoch= 1 lrate= 0.01 error= 68.09827786092596\n",
      "epoch= 2 lrate= 0.01 error= 66.38572855765064\n",
      "epoch= 3 lrate= 0.01 error= 65.4547128928136\n",
      "epoch= 4 lrate= 0.01 error= 64.90901755804029\n",
      "epoch= 5 lrate= 0.01 error= 64.57554898072021\n",
      "epoch= 6 lrate= 0.01 error= 64.3640235397645\n",
      "epoch= 7 lrate= 0.01 error= 64.22520225226559\n",
      "epoch= 8 lrate= 0.01 error= 64.13119166504026\n",
      "epoch= 9 lrate= 0.01 error= 64.06560951783077\n",
      "epoch= 10 lrate= 0.01 error= 64.01851200394398\n",
      "epoch= 11 lrate= 0.01 error= 63.983686769656266\n",
      "epoch= 12 lrate= 0.01 error= 63.957156658267415\n",
      "epoch= 13 lrate= 0.01 error= 63.936323282056264\n",
      "epoch= 14 lrate= 0.01 error= 63.91946032492567\n",
      "epoch= 15 lrate= 0.01 error= 63.90540465411178\n",
      "epoch= 16 lrate= 0.01 error= 63.89336307162672\n",
      "epoch= 17 lrate= 0.01 error= 63.88278875928495\n",
      "epoch= 18 lrate= 0.01 error= 63.87330086925613\n",
      "epoch= 19 lrate= 0.01 error= 63.86463144023793\n",
      "epoch= 20 lrate= 0.01 error= 63.85658994763323\n",
      "epoch= 21 lrate= 0.01 error= 63.849039407067146\n",
      "epoch= 22 lrate= 0.01 error= 63.84188013944067\n",
      "epoch= 23 lrate= 0.01 error= 63.83503866600277\n",
      "epoch= 24 lrate= 0.01 error= 63.82846006527424\n",
      "epoch= 25 lrate= 0.01 error= 63.82210268119674\n",
      "epoch= 26 lrate= 0.01 error= 63.8159344370097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 27 lrate= 0.01 error= 63.80993025114557\n",
      "epoch= 28 lrate= 0.01 error= 63.8040702129721\n",
      "epoch= 29 lrate= 0.01 error= 63.79833828488051\n",
      "epoch= 30 lrate= 0.01 error= 63.79272137074567\n",
      "epoch= 31 lrate= 0.01 error= 63.78720864074411\n",
      "epoch= 32 lrate= 0.01 error= 63.781791036609235\n",
      "epoch= 33 lrate= 0.01 error= 63.77646090473215\n",
      "epoch= 34 lrate= 0.01 error= 63.77121172053373\n",
      "epoch= 35 lrate= 0.01 error= 63.76603787856218\n",
      "epoch= 36 lrate= 0.01 error= 63.76093453038264\n",
      "epoch= 37 lrate= 0.01 error= 63.75589745759946\n",
      "epoch= 38 lrate= 0.01 error= 63.750922971013125\n",
      "epoch= 39 lrate= 0.01 error= 63.74600782947151\n",
      "epoch= 40 lrate= 0.01 error= 63.741149173761585\n",
      "epoch= 41 lrate= 0.01 error= 63.736344472145795\n",
      "epoch= 42 lrate= 0.01 error= 63.73159147504525\n",
      "epoch= 43 lrate= 0.01 error= 63.72688817699566\n",
      "epoch= 44 lrate= 0.01 error= 63.722232784472865\n",
      "epoch= 45 lrate= 0.01 error= 63.71762368849849\n",
      "epoch= 46 lrate= 0.01 error= 63.713059441190396\n",
      "epoch= 47 lrate= 0.01 error= 63.70853873559964\n",
      "epoch= 48 lrate= 0.01 error= 63.70406038830133\n",
      "epoch= 49 lrate= 0.01 error= 63.699623324318345\n",
      "epoch= 0 lrate= 0.01 error= 78.22164084007025\n",
      "epoch= 1 lrate= 0.01 error= 66.94787878234192\n",
      "epoch= 2 lrate= 0.01 error= 65.32523107333417\n",
      "epoch= 3 lrate= 0.01 error= 64.439950348846\n",
      "epoch= 4 lrate= 0.01 error= 63.91942424963466\n",
      "epoch= 5 lrate= 0.01 error= 63.60122415984892\n",
      "epoch= 6 lrate= 0.01 error= 63.39976517138919\n",
      "epoch= 7 lrate= 0.01 error= 63.267967261683204\n",
      "epoch= 8 lrate= 0.01 error= 63.17902603886803\n",
      "epoch= 9 lrate= 0.01 error= 63.11717575002123\n",
      "epoch= 10 lrate= 0.01 error= 63.072859396583674\n",
      "epoch= 11 lrate= 0.01 error= 63.040125071886074\n",
      "epoch= 12 lrate= 0.01 error= 63.01517803157143\n",
      "epoch= 13 lrate= 0.01 error= 62.995550057768064\n",
      "epoch= 14 lrate= 0.01 error= 62.979608465044635\n",
      "epoch= 15 lrate= 0.01 error= 62.966257722704896\n",
      "epoch= 16 lrate= 0.01 error= 62.95475360713686\n",
      "epoch= 17 lrate= 0.01 error= 62.944584961549985\n",
      "epoch= 18 lrate= 0.01 error= 62.935397114475336\n",
      "epoch= 19 lrate= 0.01 error= 62.926941540746725\n",
      "epoch= 20 lrate= 0.01 error= 62.919042365988844\n",
      "epoch= 21 lrate= 0.01 error= 62.91157385143638\n",
      "epoch= 22 lrate= 0.01 error= 62.904445128963495\n",
      "epoch= 23 lrate= 0.01 error= 62.89758977383895\n",
      "epoch= 24 lrate= 0.01 error= 62.89095863359849\n",
      "epoch= 25 lrate= 0.01 error= 62.884514864583544\n",
      "epoch= 26 lrate= 0.01 error= 62.878230474791586\n",
      "epoch= 27 lrate= 0.01 error= 62.872083900345544\n",
      "epoch= 28 lrate= 0.01 error= 62.86605829499702\n",
      "epoch= 29 lrate= 0.01 error= 62.86014031405379\n",
      "epoch= 30 lrate= 0.01 error= 62.85431924294498\n",
      "epoch= 31 lrate= 0.01 error= 62.848586367326725\n",
      "epoch= 32 lrate= 0.01 error= 62.84293451347181\n",
      "epoch= 33 lrate= 0.01 error= 62.837357709467284\n",
      "epoch= 34 lrate= 0.01 error= 62.83185093273187\n",
      "epoch= 35 lrate= 0.01 error= 62.826409919680586\n",
      "epoch= 36 lrate= 0.01 error= 62.82103102051714\n",
      "epoch= 37 lrate= 0.01 error= 62.815711087102855\n",
      "epoch= 38 lrate= 0.01 error= 62.81044738530755\n",
      "epoch= 39 lrate= 0.01 error= 62.80523752568403\n",
      "epoch= 40 lrate= 0.01 error= 62.800079408000485\n",
      "epoch= 41 lrate= 0.01 error= 62.794971176384415\n",
      "epoch= 42 lrate= 0.01 error= 62.78991118268415\n",
      "epoch= 43 lrate= 0.01 error= 62.7848979562689\n",
      "epoch= 44 lrate= 0.01 error= 62.77993017893003\n",
      "epoch= 45 lrate= 0.01 error= 62.77500666386814\n",
      "epoch= 46 lrate= 0.01 error= 62.77012633799273\n",
      "epoch= 47 lrate= 0.01 error= 62.765288226917576\n",
      "epoch= 48 lrate= 0.01 error= 62.760491442191146\n",
      "epoch= 49 lrate= 0.01 error= 62.75573517036863\n",
      "MLR RMSE: [0.12800204109373736, 0.13217526768267857, 0.12473661613079894, 0.1210386555622963, 0.12433538766319817]\n",
      "MLR RMSE Min: 0.121 MLR RMSE Max: 0.132 MLR RMSE Mean: 0.126\n",
      "zeroRR RMSE:  [0.14791399689377074, 0.15115898022836635, 0.15174431515424489, 0.14309631720132804, 0.14391423220840058]\n",
      "zeroRR RMSE Min: 0.143 zeroRR RMSE Max: 0.152 zeroRR RMSE Mean: 0.148\n"
     ]
    }
   ],
   "source": [
    "# Write your code for f through i here\n",
    "import csv\n",
    "\n",
    "# Load the data\n",
    "def load_data(filename):\n",
    "    csvTxt = csv.reader(open(filename))\n",
    "    data = []\n",
    "    for row in csvTxt:\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "def column2Float(dataset,column):\n",
    "    for instance in dataset:\n",
    "        instance[column] = float(instance[column])\n",
    "    return dataset\n",
    "\n",
    "import math\n",
    "def mean(listOfValues):\n",
    "    total = 0\n",
    "    for num in listOfValues:\n",
    "        total += num\n",
    "    return total/len(listOfValues)\n",
    "\n",
    "def zeroRR(train, test):\n",
    "    trainY = [i[-1] for i in train]\n",
    "    testY = [i[-1] for i in test]\n",
    "\n",
    "    trainYMean = mean(trainY)\n",
    "    predictions = []\n",
    "    for i in testY:\n",
    "        predictions.append(trainYMean)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rmse_eval(actual, predicted):\n",
    "    error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        error += (predicted[i] - actual[i])**2\n",
    "    error = error/len(actual)\n",
    "    error = error**0.5\n",
    "    return error\n",
    "\n",
    "def minmax(dataset):\n",
    "    listMinMax = []\n",
    "    for column in range(len(dataset[0])):\n",
    "        columnData = [dataset[i][column] for i in range(len(dataset))]\n",
    "        listMinMax.append([min(columnData), max(columnData)])\n",
    "    return listMinMax\n",
    "\n",
    "def normalize(dataset, minmax):\n",
    "    for row in range(len(dataset)):\n",
    "        for column in range(len(dataset[row])):\n",
    "            dataset[row][column] = (dataset[row][column] - minmax[column][0]) / (minmax[column][1] - minmax[column][0])\n",
    "    \n",
    "def predict(instance, coefficients):\n",
    "    y = coefficients[0]\n",
    "    for i in range(len(instance)-1):\n",
    "        y += instance[i]*coefficients[i+1]\n",
    "    return y\n",
    "\n",
    "    \n",
    "def coefficientsSGD(train, learning_rate, epochs):\n",
    "    coefficients = [0 for i in range(len(train[0]))]\n",
    "    for e in range(epochs):\n",
    "        totalError = 0\n",
    "        for instance in train:\n",
    "            predY = predict(instance, coefficients)\n",
    "            error = predY - instance[-1]\n",
    "            totalError += error**2\n",
    "            coefficients[0] -= learning_rate*error\n",
    "            for i in range(1,len(coefficients)):\n",
    "                coefficients[i] -= learning_rate*error*instance[i-1]\n",
    "        print('epoch=', e, 'lrate=', learning_rate, 'error=', totalError)\n",
    "    return coefficients\n",
    "            \n",
    "def mlr(train, test, learning_rate, epochs):\n",
    "    coefficients = coefficientsSGD(train, learning_rate, epochs)\n",
    "    predictions = []\n",
    "    for entry in test:\n",
    "        prediction = predict(entry, coefficients)\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "wineData = load_data('winequality-white.csv')\n",
    "for column in range(len(wineData[0])):\n",
    "    column2Float(wineData, column)\n",
    "minmaxWine = minmax(wineData)\n",
    "normalize(wineData, minmaxWine)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "mlr_result = evaluate_algorithm(wineData,mlr,5,rmse_eval,learning_rate,epochs)\n",
    "zeroRR_result = evaluate_algorithm(wineData,zeroRR,5,rmse_eval)\n",
    "\n",
    "print('MLR RMSE:', mlr_result)\n",
    "print('MLR RMSE Min: %.3f' % min(mlr_result), 'MLR RMSE Max: %.3f' % max(mlr_result), 'MLR RMSE Mean: %.3f' % mean(mlr_result))\n",
    "print('zeroRR RMSE: ', zeroRR_result)\n",
    "print('zeroRR RMSE Min: %.3f' % min(zeroRR_result), 'zeroRR RMSE Max: %.3f' % max(zeroRR_result), 'zeroRR RMSE Mean: %.3f' % mean(zeroRR_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Classification with Logistic Regression\n",
    "\n",
    "Everything so far has been a regression task - predicting a numeric value. We've moved on to talk about classification in class, so let's implement our first basic classifier. This is the same idea as linear regression, but we're going to predict one of two binary classes, using logistic regression.\n",
    "\n",
    "The general outline for logistic regression is the same as for multivariate linear regression. We're going to need a function to make predictions, and a function to learn coefficients. \n",
    "\n",
    "(a) The formula for making a prediction, predY, for logistic regression is:\n",
    "\n",
    "predY = 1.0 / 1.0 + e^(b0 + b1 * x1 + ... + bN * xN)\n",
    "\n",
    "Where b0 is the intercept or bias, bN is the coefficient for the input variable xN, and e is the base of the natural logarithms, or Euler's number. We can use the python math library which has an implementation of e called math.exp(x): https://docs.python.org/3/library/math.html\n",
    "\n",
    "The formula given above is an implementation of a sigmoid function (a commonly used, s-shaped function that can take any input value and produce a number between 0 and 1).\n",
    "\n",
    "We will assume there can be multiple input features (x1, x2 etc) not just a single value, and that each input feature will have a corresponding coefficient (b1, b2 etc).\n",
    "\n",
    "Write your predict function, that will take an instance, and a list of coefficients, and return a prediction. In the list of coefficients, assume coefficient[0] corresponds to b0. This will be very similar to your predict function from last week.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your predict function here\n",
    "import math\n",
    "def predictCLR(instance, coefficients):\n",
    "    power = coefficients[0]\n",
    "    for i in range(len(instance)-1):\n",
    "        power += instance[i]*coefficients[i+1]\n",
    "    predY = 1.0 / (1.0 + math.exp(-power))\n",
    "    return predY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test your predict function on the contrived dataset below. It includes TWO input variables and a class (Y) for each instance. The class is either 0 or 1.\n",
    "\n",
    "(b) Graph this data, x1 against x2, using different colored points for the two classes. Include a legend, showing which color corresponds to which class. \n",
    "\n",
    "(c) Call your predict function on each instance in the contrived data set, using the coefficient list given below. Get the predicted class from your function, and print (for each instance), the expected class, the predicted value AND the predicted class. In order to get the predicted class from the value predicted, we need to do rounding. There is a round() function that can help you. If it works correctly, you should predict the correct class of each instance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected class: 0 predicted value: 0.2987569855650975 predicted class: 0\n",
      "expected class: 0 predicted value: 0.14595105593031163 predicted class: 0\n",
      "expected class: 0 predicted value: 0.08533326519733725 predicted class: 0\n",
      "expected class: 0 predicted value: 0.21973731424800344 predicted class: 0\n",
      "expected class: 0 predicted value: 0.24705900008926596 predicted class: 0\n",
      "expected class: 1 predicted value: 0.9547021347460022 predicted class: 1\n",
      "expected class: 1 predicted value: 0.8620341905282771 predicted class: 1\n",
      "expected class: 1 predicted value: 0.9717729050420985 predicted class: 1\n",
      "expected class: 1 predicted value: 0.9992954520878627 predicted class: 1\n",
      "expected class: 1 predicted value: 0.9054893228110497 predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "# Here's the contrived data set\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "    [1.465489372,2.362125076,0],\n",
    "    [3.396561688,4.400293529,0],\n",
    "    [1.38807019,1.850220317,0],\n",
    "    [3.06407232,3.005305973,0],\n",
    "    [7.627531214,2.759262235,1],\n",
    "    [5.332441248,2.088626775,1],\n",
    "    [6.922596716,1.77106367,1],\n",
    "    [8.675418651,-0.242068655,1],\n",
    "    [7.673756466,3.508563011,1]]\n",
    "\n",
    "\n",
    "# Do the graphing here\n",
    "dataX0 = []\n",
    "dataX1 = []\n",
    "dataY0 = []\n",
    "dataY1 = []\n",
    "for i in dataset:\n",
    "    if i[2] == 0:\n",
    "        dataX0.append(i[0])\n",
    "        dataY0.append(i[1])\n",
    "    else:\n",
    "        dataX1.append(i[0])\n",
    "        dataY1.append(i[1])\n",
    "        \n",
    "plt.plot(dataX0, dataY0,'b^', label = '0 class')\n",
    "plt.plot(dataX1, dataY1,'go', label = '1 class')\n",
    "#plt.axis([0,6,0,6])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Call your predict function on the data here, using the following coefficients\n",
    "\n",
    "coeffs = [-0.406605464, 0.852573316, -1.104746259]\n",
    "\n",
    "predYData = [predictCLR(i, coeffs) for i in dataset]\n",
    "for i in range(len(predYData)):\n",
    "    print('expected class:', dataset[i][2], 'predicted value:', predYData[i], 'predicted class:', round(predYData[i]))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Above I gave you coefficients. Just as with MLR, we need to estimate the coefficients for a data set. To do that, we're going to use stochastic gradient descent. The algorithm is exactly the same as for multivariate linear regression except for the following two things.\n",
    "\n",
    "b0 is computed by:\n",
    "\n",
    "b0 = b0 + learning_rate * error * predictedY * (1.0 - predictedY)\n",
    "\n",
    "and bN is computed by:\n",
    "\n",
    "bN = bN + learning_rate * error * predictedY * (1.0 - predictedY) * xN\n",
    "\n",
    "for all coefficients b1..bN\n",
    "\n",
    "Remember, to calculate the error, we run the algorithm with default coefficients and perform prediction, then get the error by subtracting the predictedY from the actual Y value.\n",
    "\n",
    "Refer back to Assignment 4 for the complete algorithm\n",
    "\n",
    "(d) Apply your function to the contrived dataset given above, using the learning rate of 0.3, and 100 epochs. Print the resulting coefficients. I've shown my last 5 epochs of this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 lrate= 0.3 error=2.217\n",
      "epoch= 1 lrate= 0.3 error=1.613\n",
      "epoch= 2 lrate= 0.3 error=1.113\n",
      "epoch= 3 lrate= 0.3 error=0.827\n",
      "epoch= 4 lrate= 0.3 error=0.623\n",
      "epoch= 5 lrate= 0.3 error=0.494\n",
      "epoch= 6 lrate= 0.3 error=0.412\n",
      "epoch= 7 lrate= 0.3 error=0.354\n",
      "epoch= 8 lrate= 0.3 error=0.310\n",
      "epoch= 9 lrate= 0.3 error=0.276\n",
      "epoch= 10 lrate= 0.3 error=0.248\n",
      "epoch= 11 lrate= 0.3 error=0.224\n",
      "epoch= 12 lrate= 0.3 error=0.205\n",
      "epoch= 13 lrate= 0.3 error=0.189\n",
      "epoch= 14 lrate= 0.3 error=0.174\n",
      "epoch= 15 lrate= 0.3 error=0.162\n",
      "epoch= 16 lrate= 0.3 error=0.151\n",
      "epoch= 17 lrate= 0.3 error=0.142\n",
      "epoch= 18 lrate= 0.3 error=0.134\n",
      "epoch= 19 lrate= 0.3 error=0.126\n",
      "epoch= 20 lrate= 0.3 error=0.119\n",
      "epoch= 21 lrate= 0.3 error=0.113\n",
      "epoch= 22 lrate= 0.3 error=0.108\n",
      "epoch= 23 lrate= 0.3 error=0.103\n",
      "epoch= 24 lrate= 0.3 error=0.098\n",
      "epoch= 25 lrate= 0.3 error=0.094\n",
      "epoch= 26 lrate= 0.3 error=0.090\n",
      "epoch= 27 lrate= 0.3 error=0.087\n",
      "epoch= 28 lrate= 0.3 error=0.084\n",
      "epoch= 29 lrate= 0.3 error=0.080\n",
      "epoch= 30 lrate= 0.3 error=0.078\n",
      "epoch= 31 lrate= 0.3 error=0.075\n",
      "epoch= 32 lrate= 0.3 error=0.073\n",
      "epoch= 33 lrate= 0.3 error=0.070\n",
      "epoch= 34 lrate= 0.3 error=0.068\n",
      "epoch= 35 lrate= 0.3 error=0.066\n",
      "epoch= 36 lrate= 0.3 error=0.064\n",
      "epoch= 37 lrate= 0.3 error=0.062\n",
      "epoch= 38 lrate= 0.3 error=0.060\n",
      "epoch= 39 lrate= 0.3 error=0.059\n",
      "epoch= 40 lrate= 0.3 error=0.057\n",
      "epoch= 41 lrate= 0.3 error=0.056\n",
      "epoch= 42 lrate= 0.3 error=0.054\n",
      "epoch= 43 lrate= 0.3 error=0.053\n",
      "epoch= 44 lrate= 0.3 error=0.052\n",
      "epoch= 45 lrate= 0.3 error=0.051\n",
      "epoch= 46 lrate= 0.3 error=0.050\n",
      "epoch= 47 lrate= 0.3 error=0.048\n",
      "epoch= 48 lrate= 0.3 error=0.047\n",
      "epoch= 49 lrate= 0.3 error=0.046\n",
      "epoch= 50 lrate= 0.3 error=0.045\n",
      "epoch= 51 lrate= 0.3 error=0.044\n",
      "epoch= 52 lrate= 0.3 error=0.044\n",
      "epoch= 53 lrate= 0.3 error=0.043\n",
      "epoch= 54 lrate= 0.3 error=0.042\n",
      "epoch= 55 lrate= 0.3 error=0.041\n",
      "epoch= 56 lrate= 0.3 error=0.040\n",
      "epoch= 57 lrate= 0.3 error=0.040\n",
      "epoch= 58 lrate= 0.3 error=0.039\n",
      "epoch= 59 lrate= 0.3 error=0.038\n",
      "epoch= 60 lrate= 0.3 error=0.038\n",
      "epoch= 61 lrate= 0.3 error=0.037\n",
      "epoch= 62 lrate= 0.3 error=0.036\n",
      "epoch= 63 lrate= 0.3 error=0.036\n",
      "epoch= 64 lrate= 0.3 error=0.035\n",
      "epoch= 65 lrate= 0.3 error=0.035\n",
      "epoch= 66 lrate= 0.3 error=0.034\n",
      "epoch= 67 lrate= 0.3 error=0.033\n",
      "epoch= 68 lrate= 0.3 error=0.033\n",
      "epoch= 69 lrate= 0.3 error=0.032\n",
      "epoch= 70 lrate= 0.3 error=0.032\n",
      "epoch= 71 lrate= 0.3 error=0.032\n",
      "epoch= 72 lrate= 0.3 error=0.031\n",
      "epoch= 73 lrate= 0.3 error=0.031\n",
      "epoch= 74 lrate= 0.3 error=0.030\n",
      "epoch= 75 lrate= 0.3 error=0.030\n",
      "epoch= 76 lrate= 0.3 error=0.029\n",
      "epoch= 77 lrate= 0.3 error=0.029\n",
      "epoch= 78 lrate= 0.3 error=0.029\n",
      "epoch= 79 lrate= 0.3 error=0.028\n",
      "epoch= 80 lrate= 0.3 error=0.028\n",
      "epoch= 81 lrate= 0.3 error=0.027\n",
      "epoch= 82 lrate= 0.3 error=0.027\n",
      "epoch= 83 lrate= 0.3 error=0.027\n",
      "epoch= 84 lrate= 0.3 error=0.026\n",
      "epoch= 85 lrate= 0.3 error=0.026\n",
      "epoch= 86 lrate= 0.3 error=0.026\n",
      "epoch= 87 lrate= 0.3 error=0.026\n",
      "epoch= 88 lrate= 0.3 error=0.025\n",
      "epoch= 89 lrate= 0.3 error=0.025\n",
      "epoch= 90 lrate= 0.3 error=0.025\n",
      "epoch= 91 lrate= 0.3 error=0.024\n",
      "epoch= 92 lrate= 0.3 error=0.024\n",
      "epoch= 93 lrate= 0.3 error=0.024\n",
      "epoch= 94 lrate= 0.3 error=0.024\n",
      "epoch= 95 lrate= 0.3 error=0.023\n",
      "epoch= 96 lrate= 0.3 error=0.023\n",
      "epoch= 97 lrate= 0.3 error=0.023\n",
      "epoch= 98 lrate= 0.3 error=0.023\n",
      "epoch= 99 lrate= 0.3 error=0.022\n",
      "[-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n"
     ]
    }
   ],
   "source": [
    "# Write your function sgd_log(dataset, learning_rate, epochs) here\n",
    "def sgd_log(dataset, learning_rate, epochs):\n",
    "    coefficients = [0 for i in range(len(dataset[0]))]\n",
    "    for e in range(epochs):\n",
    "        totalError = 0\n",
    "        for instance in dataset:\n",
    "            predY = predictCLR(instance, coefficients)\n",
    "            error = instance[-1] - predY\n",
    "            totalError += error**2\n",
    "            coefficients[0] += learning_rate*error*predY*(1.0-predY)\n",
    "            for i in range(1,len(coefficients)):\n",
    "                coefficients[i] += learning_rate*error*predY*(1.0-predY)*instance[i-1]\n",
    "        print('epoch=', e, 'lrate=', learning_rate, 'error=%.3f' %totalError)\n",
    "    return coefficients\n",
    "\n",
    "\n",
    "\n",
    "# Call your function using the parameters given here. \n",
    "\n",
    "learning_rate = 0.3\n",
    "epochs = 100\n",
    "coeffs = sgd_log(dataset, learning_rate, epochs)\n",
    "print(coeffs)\n",
    "\n",
    "\n",
    "# Example output\n",
    "#\n",
    "#>epoch=95, lrate=0.300, error=0.023\n",
    "#>epoch=96, lrate=0.300, error=0.023\n",
    "#>epoch=97, lrate=0.300, error=0.023\n",
    "#>epoch=98, lrate=0.300, error=0.023\n",
    "#>epoch=99, lrate=0.300, error=0.022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Applying classification to real data\n",
    "\n",
    "In this final section, we'll do the following things. \n",
    "\n",
    "(a) We need a function for calculating accuracy. It will take a list of actual class values, and predicted class values. If the actual value of an instance and the predicted value of an instance are the same, increment a counter. In the end, return the value of this counter divided by the length of the actual values list, multiplied by 100 - so we are returning a percentage of the classification we got correct. This function should be called accuracy(actual, predicted).\n",
    "\n",
    "(b) We need a baseline function. Create a function called zeroRC(train, test). This function should take in the training data, and find the most common value of Y in the training data. It should then return a list of predictions the same length as the test data, containing ONLY this value that was most common in the training data. \n",
    "\n",
    "(c) I've given you the diabetes data set. You can find more about this data set here: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n",
    "\n",
    "You are going to:\n",
    "\n",
    "- load the data\n",
    "- print out some basic information about the data (number of instances, features)\n",
    "- convert each string value to float (for all columns)\n",
    "- normalize all columns in the range 0-1\n",
    "- perform a 5-fold cross validation\n",
    "    - using logistic regression\n",
    "    - using a learning rate of 0.1, and 100 epochs\n",
    "- collect predicted scores\n",
    "- print the min, max and mean scores\n",
    "- repeat the above, using zeroRC as the algorithm\n",
    "- offer me some write up of the results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances: 768 Number of Features: 9\n",
      "epoch= 0 lrate= 0.1 error=144.113\n",
      "epoch= 1 lrate= 0.1 error=137.894\n",
      "epoch= 2 lrate= 0.1 error=133.314\n",
      "epoch= 3 lrate= 0.1 error=129.598\n",
      "epoch= 4 lrate= 0.1 error=126.552\n",
      "epoch= 5 lrate= 0.1 error=124.023\n",
      "epoch= 6 lrate= 0.1 error=121.894\n",
      "epoch= 7 lrate= 0.1 error=120.079\n",
      "epoch= 8 lrate= 0.1 error=118.515\n",
      "epoch= 9 lrate= 0.1 error=117.153\n",
      "epoch= 10 lrate= 0.1 error=115.957\n",
      "epoch= 11 lrate= 0.1 error=114.899\n",
      "epoch= 12 lrate= 0.1 error=113.956\n",
      "epoch= 13 lrate= 0.1 error=113.111\n",
      "epoch= 14 lrate= 0.1 error=112.350\n",
      "epoch= 15 lrate= 0.1 error=111.661\n",
      "epoch= 16 lrate= 0.1 error=111.034\n",
      "epoch= 17 lrate= 0.1 error=110.462\n",
      "epoch= 18 lrate= 0.1 error=109.938\n",
      "epoch= 19 lrate= 0.1 error=109.456\n",
      "epoch= 20 lrate= 0.1 error=109.011\n",
      "epoch= 21 lrate= 0.1 error=108.600\n",
      "epoch= 22 lrate= 0.1 error=108.219\n",
      "epoch= 23 lrate= 0.1 error=107.865\n",
      "epoch= 24 lrate= 0.1 error=107.535\n",
      "epoch= 25 lrate= 0.1 error=107.227\n",
      "epoch= 26 lrate= 0.1 error=106.939\n",
      "epoch= 27 lrate= 0.1 error=106.670\n",
      "epoch= 28 lrate= 0.1 error=106.416\n",
      "epoch= 29 lrate= 0.1 error=106.178\n",
      "epoch= 30 lrate= 0.1 error=105.954\n",
      "epoch= 31 lrate= 0.1 error=105.743\n",
      "epoch= 32 lrate= 0.1 error=105.544\n",
      "epoch= 33 lrate= 0.1 error=105.355\n",
      "epoch= 34 lrate= 0.1 error=105.177\n",
      "epoch= 35 lrate= 0.1 error=105.008\n",
      "epoch= 36 lrate= 0.1 error=104.847\n",
      "epoch= 37 lrate= 0.1 error=104.695\n",
      "epoch= 38 lrate= 0.1 error=104.550\n",
      "epoch= 39 lrate= 0.1 error=104.413\n",
      "epoch= 40 lrate= 0.1 error=104.282\n",
      "epoch= 41 lrate= 0.1 error=104.157\n",
      "epoch= 42 lrate= 0.1 error=104.038\n",
      "epoch= 43 lrate= 0.1 error=103.924\n",
      "epoch= 44 lrate= 0.1 error=103.816\n",
      "epoch= 45 lrate= 0.1 error=103.712\n",
      "epoch= 46 lrate= 0.1 error=103.613\n",
      "epoch= 47 lrate= 0.1 error=103.518\n",
      "epoch= 48 lrate= 0.1 error=103.427\n",
      "epoch= 49 lrate= 0.1 error=103.340\n",
      "epoch= 50 lrate= 0.1 error=103.256\n",
      "epoch= 51 lrate= 0.1 error=103.176\n",
      "epoch= 52 lrate= 0.1 error=103.099\n",
      "epoch= 53 lrate= 0.1 error=103.025\n",
      "epoch= 54 lrate= 0.1 error=102.954\n",
      "epoch= 55 lrate= 0.1 error=102.885\n",
      "epoch= 56 lrate= 0.1 error=102.820\n",
      "epoch= 57 lrate= 0.1 error=102.757\n",
      "epoch= 58 lrate= 0.1 error=102.696\n",
      "epoch= 59 lrate= 0.1 error=102.637\n",
      "epoch= 60 lrate= 0.1 error=102.581\n",
      "epoch= 61 lrate= 0.1 error=102.526\n",
      "epoch= 62 lrate= 0.1 error=102.474\n",
      "epoch= 63 lrate= 0.1 error=102.423\n",
      "epoch= 64 lrate= 0.1 error=102.374\n",
      "epoch= 65 lrate= 0.1 error=102.327\n",
      "epoch= 66 lrate= 0.1 error=102.281\n",
      "epoch= 67 lrate= 0.1 error=102.237\n",
      "epoch= 68 lrate= 0.1 error=102.195\n",
      "epoch= 69 lrate= 0.1 error=102.153\n",
      "epoch= 70 lrate= 0.1 error=102.114\n",
      "epoch= 71 lrate= 0.1 error=102.075\n",
      "epoch= 72 lrate= 0.1 error=102.038\n",
      "epoch= 73 lrate= 0.1 error=102.002\n",
      "epoch= 74 lrate= 0.1 error=101.967\n",
      "epoch= 75 lrate= 0.1 error=101.933\n",
      "epoch= 76 lrate= 0.1 error=101.900\n",
      "epoch= 77 lrate= 0.1 error=101.868\n",
      "epoch= 78 lrate= 0.1 error=101.838\n",
      "epoch= 79 lrate= 0.1 error=101.808\n",
      "epoch= 80 lrate= 0.1 error=101.779\n",
      "epoch= 81 lrate= 0.1 error=101.751\n",
      "epoch= 82 lrate= 0.1 error=101.724\n",
      "epoch= 83 lrate= 0.1 error=101.697\n",
      "epoch= 84 lrate= 0.1 error=101.671\n",
      "epoch= 85 lrate= 0.1 error=101.647\n",
      "epoch= 86 lrate= 0.1 error=101.622\n",
      "epoch= 87 lrate= 0.1 error=101.599\n",
      "epoch= 88 lrate= 0.1 error=101.576\n",
      "epoch= 89 lrate= 0.1 error=101.554\n",
      "epoch= 90 lrate= 0.1 error=101.532\n",
      "epoch= 91 lrate= 0.1 error=101.511\n",
      "epoch= 92 lrate= 0.1 error=101.491\n",
      "epoch= 93 lrate= 0.1 error=101.471\n",
      "epoch= 94 lrate= 0.1 error=101.452\n",
      "epoch= 95 lrate= 0.1 error=101.433\n",
      "epoch= 96 lrate= 0.1 error=101.415\n",
      "epoch= 97 lrate= 0.1 error=101.397\n",
      "epoch= 98 lrate= 0.1 error=101.380\n",
      "epoch= 99 lrate= 0.1 error=101.363\n",
      "[-6.342662625145874, 1.3255012021085855, 5.926162813730448, -1.463360149125127, -0.4687071438694946, -0.05537631824691161, 4.435121489863233, 2.3307482537409947, 0.9813117257197178]\n",
      "epoch= 0 lrate= 0.1 error=141.043\n",
      "epoch= 1 lrate= 0.1 error=133.013\n",
      "epoch= 2 lrate= 0.1 error=127.689\n",
      "epoch= 3 lrate= 0.1 error=123.484\n",
      "epoch= 4 lrate= 0.1 error=120.123\n",
      "epoch= 5 lrate= 0.1 error=117.389\n",
      "epoch= 6 lrate= 0.1 error=115.125\n",
      "epoch= 7 lrate= 0.1 error=113.218\n",
      "epoch= 8 lrate= 0.1 error=111.586\n",
      "epoch= 9 lrate= 0.1 error=110.171\n",
      "epoch= 10 lrate= 0.1 error=108.931\n",
      "epoch= 11 lrate= 0.1 error=107.833\n",
      "epoch= 12 lrate= 0.1 error=106.854\n",
      "epoch= 13 lrate= 0.1 error=105.973\n",
      "epoch= 14 lrate= 0.1 error=105.177\n",
      "epoch= 15 lrate= 0.1 error=104.453\n",
      "epoch= 16 lrate= 0.1 error=103.792\n",
      "epoch= 17 lrate= 0.1 error=103.185\n",
      "epoch= 18 lrate= 0.1 error=102.627\n",
      "epoch= 19 lrate= 0.1 error=102.111\n",
      "epoch= 20 lrate= 0.1 error=101.632\n",
      "epoch= 21 lrate= 0.1 error=101.188\n",
      "epoch= 22 lrate= 0.1 error=100.774\n",
      "epoch= 23 lrate= 0.1 error=100.388\n",
      "epoch= 24 lrate= 0.1 error=100.026\n",
      "epoch= 25 lrate= 0.1 error=99.688\n",
      "epoch= 26 lrate= 0.1 error=99.370\n",
      "epoch= 27 lrate= 0.1 error=99.070\n",
      "epoch= 28 lrate= 0.1 error=98.788\n",
      "epoch= 29 lrate= 0.1 error=98.523\n",
      "epoch= 30 lrate= 0.1 error=98.272\n",
      "epoch= 31 lrate= 0.1 error=98.034\n",
      "epoch= 32 lrate= 0.1 error=97.809\n",
      "epoch= 33 lrate= 0.1 error=97.596\n",
      "epoch= 34 lrate= 0.1 error=97.394\n",
      "epoch= 35 lrate= 0.1 error=97.202\n",
      "epoch= 36 lrate= 0.1 error=97.019\n",
      "epoch= 37 lrate= 0.1 error=96.846\n",
      "epoch= 38 lrate= 0.1 error=96.680\n",
      "epoch= 39 lrate= 0.1 error=96.523\n",
      "epoch= 40 lrate= 0.1 error=96.372\n",
      "epoch= 41 lrate= 0.1 error=96.229\n",
      "epoch= 42 lrate= 0.1 error=96.092\n",
      "epoch= 43 lrate= 0.1 error=95.961\n",
      "epoch= 44 lrate= 0.1 error=95.835\n",
      "epoch= 45 lrate= 0.1 error=95.715\n",
      "epoch= 46 lrate= 0.1 error=95.601\n",
      "epoch= 47 lrate= 0.1 error=95.491\n",
      "epoch= 48 lrate= 0.1 error=95.385\n",
      "epoch= 49 lrate= 0.1 error=95.284\n",
      "epoch= 50 lrate= 0.1 error=95.187\n",
      "epoch= 51 lrate= 0.1 error=95.094\n",
      "epoch= 52 lrate= 0.1 error=95.004\n",
      "epoch= 53 lrate= 0.1 error=94.918\n",
      "epoch= 54 lrate= 0.1 error=94.835\n",
      "epoch= 55 lrate= 0.1 error=94.756\n",
      "epoch= 56 lrate= 0.1 error=94.679\n",
      "epoch= 57 lrate= 0.1 error=94.605\n",
      "epoch= 58 lrate= 0.1 error=94.534\n",
      "epoch= 59 lrate= 0.1 error=94.466\n",
      "epoch= 60 lrate= 0.1 error=94.400\n",
      "epoch= 61 lrate= 0.1 error=94.336\n",
      "epoch= 62 lrate= 0.1 error=94.275\n",
      "epoch= 63 lrate= 0.1 error=94.216\n",
      "epoch= 64 lrate= 0.1 error=94.159\n",
      "epoch= 65 lrate= 0.1 error=94.103\n",
      "epoch= 66 lrate= 0.1 error=94.050\n",
      "epoch= 67 lrate= 0.1 error=93.999\n",
      "epoch= 68 lrate= 0.1 error=93.949\n",
      "epoch= 69 lrate= 0.1 error=93.901\n",
      "epoch= 70 lrate= 0.1 error=93.854\n",
      "epoch= 71 lrate= 0.1 error=93.809\n",
      "epoch= 72 lrate= 0.1 error=93.766\n",
      "epoch= 73 lrate= 0.1 error=93.724\n",
      "epoch= 74 lrate= 0.1 error=93.683\n",
      "epoch= 75 lrate= 0.1 error=93.643\n",
      "epoch= 76 lrate= 0.1 error=93.605\n",
      "epoch= 77 lrate= 0.1 error=93.568\n",
      "epoch= 78 lrate= 0.1 error=93.532\n",
      "epoch= 79 lrate= 0.1 error=93.497\n",
      "epoch= 80 lrate= 0.1 error=93.463\n",
      "epoch= 81 lrate= 0.1 error=93.431\n",
      "epoch= 82 lrate= 0.1 error=93.399\n",
      "epoch= 83 lrate= 0.1 error=93.368\n",
      "epoch= 84 lrate= 0.1 error=93.338\n",
      "epoch= 85 lrate= 0.1 error=93.309\n",
      "epoch= 86 lrate= 0.1 error=93.281\n",
      "epoch= 87 lrate= 0.1 error=93.253\n",
      "epoch= 88 lrate= 0.1 error=93.227\n",
      "epoch= 89 lrate= 0.1 error=93.201\n",
      "epoch= 90 lrate= 0.1 error=93.176\n",
      "epoch= 91 lrate= 0.1 error=93.151\n",
      "epoch= 92 lrate= 0.1 error=93.128\n",
      "epoch= 93 lrate= 0.1 error=93.105\n",
      "epoch= 94 lrate= 0.1 error=93.082\n",
      "epoch= 95 lrate= 0.1 error=93.060\n",
      "epoch= 96 lrate= 0.1 error=93.039\n",
      "epoch= 97 lrate= 0.1 error=93.018\n",
      "epoch= 98 lrate= 0.1 error=92.998\n",
      "epoch= 99 lrate= 0.1 error=92.979\n",
      "[-6.878604363000969, 2.4936047220457525, 6.182686441136319, -0.9464637066270445, -0.11788043161853758, -0.8122052527112646, 4.10790461869173, 2.514065829506098, 0.5983600542416614]\n",
      "epoch= 0 lrate= 0.1 error=140.503\n",
      "epoch= 1 lrate= 0.1 error=131.694\n",
      "epoch= 2 lrate= 0.1 error=125.681\n",
      "epoch= 3 lrate= 0.1 error=120.899\n",
      "epoch= 4 lrate= 0.1 error=117.065\n",
      "epoch= 5 lrate= 0.1 error=113.945\n",
      "epoch= 6 lrate= 0.1 error=111.363\n",
      "epoch= 7 lrate= 0.1 error=109.192\n",
      "epoch= 8 lrate= 0.1 error=107.341\n",
      "epoch= 9 lrate= 0.1 error=105.743\n",
      "epoch= 10 lrate= 0.1 error=104.347\n",
      "epoch= 11 lrate= 0.1 error=103.117\n",
      "epoch= 12 lrate= 0.1 error=102.024\n",
      "epoch= 13 lrate= 0.1 error=101.045\n",
      "epoch= 14 lrate= 0.1 error=100.163\n",
      "epoch= 15 lrate= 0.1 error=99.363\n",
      "epoch= 16 lrate= 0.1 error=98.634\n",
      "epoch= 17 lrate= 0.1 error=97.967\n",
      "epoch= 18 lrate= 0.1 error=97.353\n",
      "epoch= 19 lrate= 0.1 error=96.787\n",
      "epoch= 20 lrate= 0.1 error=96.262\n",
      "epoch= 21 lrate= 0.1 error=95.775\n",
      "epoch= 22 lrate= 0.1 error=95.321\n",
      "epoch= 23 lrate= 0.1 error=94.896\n",
      "epoch= 24 lrate= 0.1 error=94.499\n",
      "epoch= 25 lrate= 0.1 error=94.126\n",
      "epoch= 26 lrate= 0.1 error=93.775\n",
      "epoch= 27 lrate= 0.1 error=93.444\n",
      "epoch= 28 lrate= 0.1 error=93.131\n",
      "epoch= 29 lrate= 0.1 error=92.836\n",
      "epoch= 30 lrate= 0.1 error=92.556\n",
      "epoch= 31 lrate= 0.1 error=92.290\n",
      "epoch= 32 lrate= 0.1 error=92.038\n",
      "epoch= 33 lrate= 0.1 error=91.798\n",
      "epoch= 34 lrate= 0.1 error=91.570\n",
      "epoch= 35 lrate= 0.1 error=91.352\n",
      "epoch= 36 lrate= 0.1 error=91.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 37 lrate= 0.1 error=90.946\n",
      "epoch= 38 lrate= 0.1 error=90.757\n",
      "epoch= 39 lrate= 0.1 error=90.575\n",
      "epoch= 40 lrate= 0.1 error=90.402\n",
      "epoch= 41 lrate= 0.1 error=90.235\n",
      "epoch= 42 lrate= 0.1 error=90.076\n",
      "epoch= 43 lrate= 0.1 error=89.923\n",
      "epoch= 44 lrate= 0.1 error=89.776\n",
      "epoch= 45 lrate= 0.1 error=89.634\n",
      "epoch= 46 lrate= 0.1 error=89.498\n",
      "epoch= 47 lrate= 0.1 error=89.368\n",
      "epoch= 48 lrate= 0.1 error=89.242\n",
      "epoch= 49 lrate= 0.1 error=89.121\n",
      "epoch= 50 lrate= 0.1 error=89.004\n",
      "epoch= 51 lrate= 0.1 error=88.891\n",
      "epoch= 52 lrate= 0.1 error=88.783\n",
      "epoch= 53 lrate= 0.1 error=88.678\n",
      "epoch= 54 lrate= 0.1 error=88.577\n",
      "epoch= 55 lrate= 0.1 error=88.480\n",
      "epoch= 56 lrate= 0.1 error=88.385\n",
      "epoch= 57 lrate= 0.1 error=88.294\n",
      "epoch= 58 lrate= 0.1 error=88.206\n",
      "epoch= 59 lrate= 0.1 error=88.121\n",
      "epoch= 60 lrate= 0.1 error=88.038\n",
      "epoch= 61 lrate= 0.1 error=87.958\n",
      "epoch= 62 lrate= 0.1 error=87.881\n",
      "epoch= 63 lrate= 0.1 error=87.806\n",
      "epoch= 64 lrate= 0.1 error=87.733\n",
      "epoch= 65 lrate= 0.1 error=87.663\n",
      "epoch= 66 lrate= 0.1 error=87.595\n",
      "epoch= 67 lrate= 0.1 error=87.529\n",
      "epoch= 68 lrate= 0.1 error=87.465\n",
      "epoch= 69 lrate= 0.1 error=87.402\n",
      "epoch= 70 lrate= 0.1 error=87.342\n",
      "epoch= 71 lrate= 0.1 error=87.283\n",
      "epoch= 72 lrate= 0.1 error=87.226\n",
      "epoch= 73 lrate= 0.1 error=87.171\n",
      "epoch= 74 lrate= 0.1 error=87.117\n",
      "epoch= 75 lrate= 0.1 error=87.065\n",
      "epoch= 76 lrate= 0.1 error=87.014\n",
      "epoch= 77 lrate= 0.1 error=86.964\n",
      "epoch= 78 lrate= 0.1 error=86.916\n",
      "epoch= 79 lrate= 0.1 error=86.870\n",
      "epoch= 80 lrate= 0.1 error=86.824\n",
      "epoch= 81 lrate= 0.1 error=86.780\n",
      "epoch= 82 lrate= 0.1 error=86.737\n",
      "epoch= 83 lrate= 0.1 error=86.695\n",
      "epoch= 84 lrate= 0.1 error=86.654\n",
      "epoch= 85 lrate= 0.1 error=86.614\n",
      "epoch= 86 lrate= 0.1 error=86.575\n",
      "epoch= 87 lrate= 0.1 error=86.537\n",
      "epoch= 88 lrate= 0.1 error=86.501\n",
      "epoch= 89 lrate= 0.1 error=86.465\n",
      "epoch= 90 lrate= 0.1 error=86.430\n",
      "epoch= 91 lrate= 0.1 error=86.395\n",
      "epoch= 92 lrate= 0.1 error=86.362\n",
      "epoch= 93 lrate= 0.1 error=86.330\n",
      "epoch= 94 lrate= 0.1 error=86.298\n",
      "epoch= 95 lrate= 0.1 error=86.267\n",
      "epoch= 96 lrate= 0.1 error=86.237\n",
      "epoch= 97 lrate= 0.1 error=86.207\n",
      "epoch= 98 lrate= 0.1 error=86.178\n",
      "epoch= 99 lrate= 0.1 error=86.150\n",
      "[-7.181911553296116, 1.8672638441615745, 6.85126326565694, -1.6430695512287077, 0.09268741562081394, -0.6947538976147828, 4.595732309366056, 2.767698134896744, 0.9991273696209975]\n",
      "epoch= 0 lrate= 0.1 error=142.440\n",
      "epoch= 1 lrate= 0.1 error=135.343\n",
      "epoch= 2 lrate= 0.1 error=130.632\n",
      "epoch= 3 lrate= 0.1 error=126.835\n",
      "epoch= 4 lrate= 0.1 error=123.739\n",
      "epoch= 5 lrate= 0.1 error=121.178\n",
      "epoch= 6 lrate= 0.1 error=119.026\n",
      "epoch= 7 lrate= 0.1 error=117.190\n",
      "epoch= 8 lrate= 0.1 error=115.604\n",
      "epoch= 9 lrate= 0.1 error=114.219\n",
      "epoch= 10 lrate= 0.1 error=112.999\n",
      "epoch= 11 lrate= 0.1 error=111.915\n",
      "epoch= 12 lrate= 0.1 error=110.946\n",
      "epoch= 13 lrate= 0.1 error=110.074\n",
      "epoch= 14 lrate= 0.1 error=109.287\n",
      "epoch= 15 lrate= 0.1 error=108.572\n",
      "epoch= 16 lrate= 0.1 error=107.920\n",
      "epoch= 17 lrate= 0.1 error=107.325\n",
      "epoch= 18 lrate= 0.1 error=106.778\n",
      "epoch= 19 lrate= 0.1 error=106.274\n",
      "epoch= 20 lrate= 0.1 error=105.810\n",
      "epoch= 21 lrate= 0.1 error=105.380\n",
      "epoch= 22 lrate= 0.1 error=104.982\n",
      "epoch= 23 lrate= 0.1 error=104.611\n",
      "epoch= 24 lrate= 0.1 error=104.266\n",
      "epoch= 25 lrate= 0.1 error=103.944\n",
      "epoch= 26 lrate= 0.1 error=103.643\n",
      "epoch= 27 lrate= 0.1 error=103.361\n",
      "epoch= 28 lrate= 0.1 error=103.097\n",
      "epoch= 29 lrate= 0.1 error=102.849\n",
      "epoch= 30 lrate= 0.1 error=102.616\n",
      "epoch= 31 lrate= 0.1 error=102.396\n",
      "epoch= 32 lrate= 0.1 error=102.189\n",
      "epoch= 33 lrate= 0.1 error=101.993\n",
      "epoch= 34 lrate= 0.1 error=101.808\n",
      "epoch= 35 lrate= 0.1 error=101.634\n",
      "epoch= 36 lrate= 0.1 error=101.468\n",
      "epoch= 37 lrate= 0.1 error=101.311\n",
      "epoch= 38 lrate= 0.1 error=101.162\n",
      "epoch= 39 lrate= 0.1 error=101.021\n",
      "epoch= 40 lrate= 0.1 error=100.887\n",
      "epoch= 41 lrate= 0.1 error=100.759\n",
      "epoch= 42 lrate= 0.1 error=100.637\n",
      "epoch= 43 lrate= 0.1 error=100.521\n",
      "epoch= 44 lrate= 0.1 error=100.411\n",
      "epoch= 45 lrate= 0.1 error=100.306\n",
      "epoch= 46 lrate= 0.1 error=100.205\n",
      "epoch= 47 lrate= 0.1 error=100.109\n",
      "epoch= 48 lrate= 0.1 error=100.018\n",
      "epoch= 49 lrate= 0.1 error=99.930\n",
      "epoch= 50 lrate= 0.1 error=99.846\n",
      "epoch= 51 lrate= 0.1 error=99.766\n",
      "epoch= 52 lrate= 0.1 error=99.689\n",
      "epoch= 53 lrate= 0.1 error=99.615\n",
      "epoch= 54 lrate= 0.1 error=99.544\n",
      "epoch= 55 lrate= 0.1 error=99.477\n",
      "epoch= 56 lrate= 0.1 error=99.412\n",
      "epoch= 57 lrate= 0.1 error=99.349\n",
      "epoch= 58 lrate= 0.1 error=99.289\n",
      "epoch= 59 lrate= 0.1 error=99.232\n",
      "epoch= 60 lrate= 0.1 error=99.176\n",
      "epoch= 61 lrate= 0.1 error=99.123\n",
      "epoch= 62 lrate= 0.1 error=99.072\n",
      "epoch= 63 lrate= 0.1 error=99.023\n",
      "epoch= 64 lrate= 0.1 error=98.975\n",
      "epoch= 65 lrate= 0.1 error=98.930\n",
      "epoch= 66 lrate= 0.1 error=98.886\n",
      "epoch= 67 lrate= 0.1 error=98.843\n",
      "epoch= 68 lrate= 0.1 error=98.803\n",
      "epoch= 69 lrate= 0.1 error=98.763\n",
      "epoch= 70 lrate= 0.1 error=98.725\n",
      "epoch= 71 lrate= 0.1 error=98.689\n",
      "epoch= 72 lrate= 0.1 error=98.653\n",
      "epoch= 73 lrate= 0.1 error=98.619\n",
      "epoch= 74 lrate= 0.1 error=98.586\n",
      "epoch= 75 lrate= 0.1 error=98.554\n",
      "epoch= 76 lrate= 0.1 error=98.524\n",
      "epoch= 77 lrate= 0.1 error=98.494\n",
      "epoch= 78 lrate= 0.1 error=98.465\n",
      "epoch= 79 lrate= 0.1 error=98.437\n",
      "epoch= 80 lrate= 0.1 error=98.410\n",
      "epoch= 81 lrate= 0.1 error=98.384\n",
      "epoch= 82 lrate= 0.1 error=98.359\n",
      "epoch= 83 lrate= 0.1 error=98.335\n",
      "epoch= 84 lrate= 0.1 error=98.311\n",
      "epoch= 85 lrate= 0.1 error=98.289\n",
      "epoch= 86 lrate= 0.1 error=98.266\n",
      "epoch= 87 lrate= 0.1 error=98.245\n",
      "epoch= 88 lrate= 0.1 error=98.224\n",
      "epoch= 89 lrate= 0.1 error=98.204\n",
      "epoch= 90 lrate= 0.1 error=98.185\n",
      "epoch= 91 lrate= 0.1 error=98.166\n",
      "epoch= 92 lrate= 0.1 error=98.147\n",
      "epoch= 93 lrate= 0.1 error=98.130\n",
      "epoch= 94 lrate= 0.1 error=98.112\n",
      "epoch= 95 lrate= 0.1 error=98.096\n",
      "epoch= 96 lrate= 0.1 error=98.080\n",
      "epoch= 97 lrate= 0.1 error=98.064\n",
      "epoch= 98 lrate= 0.1 error=98.049\n",
      "epoch= 99 lrate= 0.1 error=98.034\n",
      "[-6.37744148748365, 1.4875685258772502, 6.68421983466059, -1.4547849182717003, 0.2895266736033902, -0.7803576978454224, 3.599616061120318, 1.4033354300747716, 0.6607923524197946]\n",
      "epoch= 0 lrate= 0.1 error=142.072\n",
      "epoch= 1 lrate= 0.1 error=134.720\n",
      "epoch= 2 lrate= 0.1 error=129.700\n",
      "epoch= 3 lrate= 0.1 error=125.598\n",
      "epoch= 4 lrate= 0.1 error=122.217\n",
      "epoch= 5 lrate= 0.1 error=119.399\n",
      "epoch= 6 lrate= 0.1 error=117.018\n",
      "epoch= 7 lrate= 0.1 error=114.983\n",
      "epoch= 8 lrate= 0.1 error=113.224\n",
      "epoch= 9 lrate= 0.1 error=111.690\n",
      "epoch= 10 lrate= 0.1 error=110.340\n",
      "epoch= 11 lrate= 0.1 error=109.144\n",
      "epoch= 12 lrate= 0.1 error=108.077\n",
      "epoch= 13 lrate= 0.1 error=107.120\n",
      "epoch= 14 lrate= 0.1 error=106.258\n",
      "epoch= 15 lrate= 0.1 error=105.478\n",
      "epoch= 16 lrate= 0.1 error=104.768\n",
      "epoch= 17 lrate= 0.1 error=104.120\n",
      "epoch= 18 lrate= 0.1 error=103.527\n",
      "epoch= 19 lrate= 0.1 error=102.982\n",
      "epoch= 20 lrate= 0.1 error=102.480\n",
      "epoch= 21 lrate= 0.1 error=102.016\n",
      "epoch= 22 lrate= 0.1 error=101.586\n",
      "epoch= 23 lrate= 0.1 error=101.187\n",
      "epoch= 24 lrate= 0.1 error=100.815\n",
      "epoch= 25 lrate= 0.1 error=100.469\n",
      "epoch= 26 lrate= 0.1 error=100.145\n",
      "epoch= 27 lrate= 0.1 error=99.841\n",
      "epoch= 28 lrate= 0.1 error=99.557\n",
      "epoch= 29 lrate= 0.1 error=99.289\n",
      "epoch= 30 lrate= 0.1 error=99.037\n",
      "epoch= 31 lrate= 0.1 error=98.800\n",
      "epoch= 32 lrate= 0.1 error=98.576\n",
      "epoch= 33 lrate= 0.1 error=98.365\n",
      "epoch= 34 lrate= 0.1 error=98.165\n",
      "epoch= 35 lrate= 0.1 error=97.975\n",
      "epoch= 36 lrate= 0.1 error=97.795\n",
      "epoch= 37 lrate= 0.1 error=97.624\n",
      "epoch= 38 lrate= 0.1 error=97.462\n",
      "epoch= 39 lrate= 0.1 error=97.307\n",
      "epoch= 40 lrate= 0.1 error=97.160\n",
      "epoch= 41 lrate= 0.1 error=97.020\n",
      "epoch= 42 lrate= 0.1 error=96.886\n",
      "epoch= 43 lrate= 0.1 error=96.759\n",
      "epoch= 44 lrate= 0.1 error=96.637\n",
      "epoch= 45 lrate= 0.1 error=96.520\n",
      "epoch= 46 lrate= 0.1 error=96.408\n",
      "epoch= 47 lrate= 0.1 error=96.301\n",
      "epoch= 48 lrate= 0.1 error=96.199\n",
      "epoch= 49 lrate= 0.1 error=96.100\n",
      "epoch= 50 lrate= 0.1 error=96.006\n",
      "epoch= 51 lrate= 0.1 error=95.915\n",
      "epoch= 52 lrate= 0.1 error=95.828\n",
      "epoch= 53 lrate= 0.1 error=95.744\n",
      "epoch= 54 lrate= 0.1 error=95.664\n",
      "epoch= 55 lrate= 0.1 error=95.586\n",
      "epoch= 56 lrate= 0.1 error=95.512\n",
      "epoch= 57 lrate= 0.1 error=95.440\n",
      "epoch= 58 lrate= 0.1 error=95.371\n",
      "epoch= 59 lrate= 0.1 error=95.304\n",
      "epoch= 60 lrate= 0.1 error=95.239\n",
      "epoch= 61 lrate= 0.1 error=95.177\n",
      "epoch= 62 lrate= 0.1 error=95.117\n",
      "epoch= 63 lrate= 0.1 error=95.059\n",
      "epoch= 64 lrate= 0.1 error=95.003\n",
      "epoch= 65 lrate= 0.1 error=94.949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 66 lrate= 0.1 error=94.896\n",
      "epoch= 67 lrate= 0.1 error=94.846\n",
      "epoch= 68 lrate= 0.1 error=94.797\n",
      "epoch= 69 lrate= 0.1 error=94.749\n",
      "epoch= 70 lrate= 0.1 error=94.703\n",
      "epoch= 71 lrate= 0.1 error=94.658\n",
      "epoch= 72 lrate= 0.1 error=94.615\n",
      "epoch= 73 lrate= 0.1 error=94.573\n",
      "epoch= 74 lrate= 0.1 error=94.533\n",
      "epoch= 75 lrate= 0.1 error=94.493\n",
      "epoch= 76 lrate= 0.1 error=94.455\n",
      "epoch= 77 lrate= 0.1 error=94.418\n",
      "epoch= 78 lrate= 0.1 error=94.382\n",
      "epoch= 79 lrate= 0.1 error=94.347\n",
      "epoch= 80 lrate= 0.1 error=94.313\n",
      "epoch= 81 lrate= 0.1 error=94.279\n",
      "epoch= 82 lrate= 0.1 error=94.247\n",
      "epoch= 83 lrate= 0.1 error=94.216\n",
      "epoch= 84 lrate= 0.1 error=94.186\n",
      "epoch= 85 lrate= 0.1 error=94.156\n",
      "epoch= 86 lrate= 0.1 error=94.127\n",
      "epoch= 87 lrate= 0.1 error=94.099\n",
      "epoch= 88 lrate= 0.1 error=94.072\n",
      "epoch= 89 lrate= 0.1 error=94.045\n",
      "epoch= 90 lrate= 0.1 error=94.019\n",
      "epoch= 91 lrate= 0.1 error=93.994\n",
      "epoch= 92 lrate= 0.1 error=93.970\n",
      "epoch= 93 lrate= 0.1 error=93.946\n",
      "epoch= 94 lrate= 0.1 error=93.922\n",
      "epoch= 95 lrate= 0.1 error=93.900\n",
      "epoch= 96 lrate= 0.1 error=93.878\n",
      "epoch= 97 lrate= 0.1 error=93.856\n",
      "epoch= 98 lrate= 0.1 error=93.835\n",
      "epoch= 99 lrate= 0.1 error=93.814\n",
      "[-6.7384172075265045, 2.09785505031201, 6.442087857900586, -1.3648731793993831, -0.44782227158517923, 0.3957118790764493, 4.5759519992565645, 2.3200480969202824, 0.08072107906660733]\n",
      "Log Reg: [83.66013071895425, 73.85620915032679, 66.66666666666667, 78.43137254901961, 75.0]\n",
      "Log Reg Min: 66.667 Log Reg Max: 83.660 Log Reg Mean: 75.523\n",
      "zeroRC:  [60.78431372549019, 69.28104575163398, 62.091503267973856, 66.66666666666667, 66.66666666666667]\n",
      "zeroRC Min: 60.784 zeroRC Max: 69.281 zeroRC Mean: 65.098\n"
     ]
    }
   ],
   "source": [
    "# Do all the code here\n",
    "import statistics\n",
    "\n",
    "def accuracy(actual, predicted):\n",
    "    counter = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            counter += 1\n",
    "    return counter*100/len(actual)\n",
    "\n",
    "def zeroRC(train, test):\n",
    "    trainY = [i[-1] for i in train]\n",
    "    dataMode = statistics.mode(trainY)\n",
    "    return [dataMode for i in test]\n",
    "\n",
    "def clr(train, test, learning_rate, epochs):\n",
    "    coefficients = sgd_log(train, learning_rate, epochs)\n",
    "    print(coefficients)\n",
    "    predictions = []\n",
    "    for entry in test:\n",
    "        prediction = predictCLR(entry, coefficients)\n",
    "        predictions.append(round(prediction))\n",
    "    return predictions\n",
    "\n",
    "diabetesData = load_data('pima-indians-diabetes.csv')\n",
    "print('Number of Instances:', len(diabetesData), 'Number of Features:', len(diabetesData[0]))\n",
    "for column in range(len(diabetesData[0])):\n",
    "    column2Float(diabetesData, column)\n",
    "\n",
    "\n",
    "minmaxDiabetes = minmax(diabetesData)\n",
    "normalize(diabetesData, minmaxDiabetes)\n",
    "\n",
    "testData = diabetesData\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "numFolds = 5\n",
    "sgd_result = evaluate_algorithm(testData,clr, numFolds,accuracy,learning_rate,epochs)\n",
    "zeroRC_result = evaluate_algorithm(testData,zeroRC, numFolds,accuracy)\n",
    "print('Log Reg:', sgd_result)\n",
    "print('Log Reg Min: %.3f' % min(sgd_result), 'Log Reg Max: %.3f' % max(sgd_result), 'Log Reg Mean: %.3f' % mean(sgd_result))\n",
    "print('zeroRC: ', zeroRC_result)\n",
    "print('zeroRC Min: %.3f' % min(zeroRC_result), 'zeroRC Max: %.3f' % max(zeroRC_result), 'zeroRC Mean: %.3f' % mean(zeroRC_result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write up your observations on the experiment here\n",
    "\n",
    "Zero RC works worse than the linear regression classifier. This is because the data is split around 65% of the time one class, 35% of the time the other class so just choosing the most common class only has a maximum accuracy of 65%. The linear regression classifier has a median accuracy around 10% better than Zero RC but I would still not trust it to make a diagnosis. We also do not know from this what the false positive and false negative rates are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
