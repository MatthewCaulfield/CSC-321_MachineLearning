{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-321: Data Mining and Machine Learning\n",
    "# Matthew Caulfield\n",
    "## Assignment 8: K-Nearest Neighbor\n",
    "\n",
    "### Part 1: Implementation\n",
    "\n",
    "For this assignment, I'm going to let you break down the implementation as you see fit. You're going to implement KNN. In brief, this means:\n",
    "\n",
    "- calculating euclidean distance between a test instance, and a training instance\n",
    "- iterating through all the training instances, and storing distances in a list\n",
    "- returning the k nearest neighbors\n",
    "- making a prediction by choosing the class that appears the most in the k nearest neighbors\n",
    "\n",
    "To calculate euclidean distance between an instance x1 and an instance x2, we need to iterate through the features (excluding the class) of the two instances (for i features) and for each take the difference of (x1[i]) - (x2[i]), squaring that difference, and summing over all features. At the end, take the square root of the total. In other words:\n",
    "\n",
    "$$distance=\\sqrt{\\sum_{i=1}^n (x1_{i} - x2_{i})^2}$$\n",
    "\n",
    "I didn't really need to include this equation, but now you have an example of embedding a latex equation inside markdown. You're welcome.\n",
    "\n",
    "I would strongly suggest you follow the implementation outline of previous algorithms in terms of the functions you use, but I'm leaving it up to you.\n",
    "\n",
    "Below is the same contrived dataset you've used before. If your code works, you should be able to take an instance of this data, and compare it to all the others (including itself, where the distance SHOULD be 0). You should be able to select the k-nearest neighbors, and make a prediction based on the most frequently occuring class.\n",
    "\n",
    "Make sure you create a knn function that takes a training set, a test set and a value for k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance test [0.0, 0.6185116050573538, 2.2971549072793094, 2.355481259443775, 1.2353710961872457, 4.672743225343651, 2.6412435314940947, 5.7814327983643325, 4.532951443185023, 4.799917756676257]\n"
     ]
    }
   ],
   "source": [
    "# Contrived data set\n",
    "import operator\n",
    "\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "    [3.110073483,1.781539638,0],\n",
    "    [1.343808831,3.368360954,0],\n",
    "    [3.582294042,4.67917911,0],\n",
    "    [2.280362439,2.866990263,0],\n",
    "    [7.423436942,4.696522875,1],\n",
    "    [5.745051997,3.533989803,1],\n",
    "    [9.172168622,2.511101045,1],\n",
    "    [7.792783481,3.424088941,1],\n",
    "    [7.939820817,0.791637231,1]]\n",
    "\n",
    "def euclidDistance(x1, x2):\n",
    "    distance = 0\n",
    "    for i in range(len(x1)-1):\n",
    "        distance += (x1[i] - x2[i])**2\n",
    "    totalDistance = distance**(0.5)\n",
    "    return totalDistance\n",
    "\n",
    "test = []\n",
    "for j in dataset:\n",
    "    test.append(euclidDistance(dataset[0],j))\n",
    "    \n",
    "print('distance test', test)\n",
    "\n",
    "def kClosest(aList, k):\n",
    "    kList = []\n",
    "    copyList = aList[:]\n",
    "    for i in range(k):\n",
    "        currMin = min(copyList)\n",
    "        #print(currMin)\n",
    "        kList.append(currMin)\n",
    "        #print(kList)\n",
    "        #print(aList)\n",
    "        copyList.pop(copyList.index(currMin))\n",
    "    return kList\n",
    "\n",
    "        \n",
    "def knn(train, test, k):\n",
    "    results = []\n",
    "    for i in range(len(test)):\n",
    "        testDistances = []\n",
    "        for j in range(len(train)):\n",
    "            testDistances.append(euclidDistance(test[i], train[j]))\n",
    "        kClosestList = kClosest(testDistances, k)\n",
    "        nearistNeighbors = [train[testDistances.index(i)] for i in kClosestList]\n",
    "        nearistClasses = [i[-1] for i in nearistNeighbors]\n",
    "        results.append(max(set(nearistClasses), key=nearistClasses.count))\n",
    "    return results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Working with real data\n",
    "\n",
    "Apply the KNN algorithm above to the abalone data set. You can find more about it here: http://archive.ics.uci.edu/ml/datasets/Abalone\n",
    "\n",
    "You will need to make the class value into an integer class. Run a 5-fold cross-validation, with k set as 5. Also run a classification baseline. Report on classification accuracy, and write up some results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances: 4177 Number of Features: 9\n",
      "abaloneData first row [1, 0.455, 0.365, 0.095, 0.514, 0.2245, 0.101, 0.15, 15.0]\n",
      "knn: [24.910179640718564, 21.79640718562874, 23.592814371257486, 21.676646706586826, 23.416965352449225]\n",
      "knn Min: 21.677 knn Max: 24.910 knn Mean: 23.079\n",
      "zeroRC:  [15.568862275449101, 17.604790419161677, 15.688622754491018, 17.604790419161677, 16.009557945041816]\n",
      "zeroRC Min: 15.569 zeroRC Max: 17.605 zeroRC Mean: 16.495\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def mean(listOfValues):\n",
    "    total = 0\n",
    "    for num in listOfValues:\n",
    "        total += num\n",
    "    return total/len(listOfValues)\n",
    "\n",
    "def variance(listOfValues, meanValue):\n",
    "    total = 0\n",
    "    for num in listOfValues:\n",
    "       total +=  (num - meanValue)**2/(len(listOfValues)-1)\n",
    "    return total\n",
    "\n",
    "def load_data(filename):\n",
    "    csvTxt = csv.reader(open(filename))\n",
    "    data = []\n",
    "    for row in csvTxt:\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "def column2Float(dataset,column):\n",
    "    for instance in dataset:\n",
    "        instance[column] = float(instance[column])\n",
    "    return dataset\n",
    "\n",
    "def rmse_eval(actual, predicted):\n",
    "    error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        error += (predicted[i] - actual[i])**2\n",
    "    error = error/len(actual)\n",
    "    error = error**0.5\n",
    "    return error\n",
    "\n",
    "def minmax(dataset):\n",
    "    listMinMax = []\n",
    "    for column in range(len(dataset[0])):\n",
    "        columnData = [dataset[i][column] for i in range(len(dataset))]\n",
    "        listMinMax.append([min(columnData), max(columnData)])\n",
    "    return listMinMax\n",
    "\n",
    "def normalize(dataset, minmax):\n",
    "    for row in range(len(dataset)):\n",
    "        for column in range(len(dataset[row])):\n",
    "            dataset[row][column] = (dataset[row][column] - minmax[column][0]) / (minmax[column][1] - minmax[column][0])\n",
    "\n",
    "def accuracy(actual, predicted):\n",
    "    counter = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            counter += 1\n",
    "    return counter*100/len(actual)\n",
    "\n",
    "def zeroRC(train, test):\n",
    "    trainY = [i[-1] for i in train]\n",
    "    count = Counter(trainY)\n",
    "    dataMode = count.most_common(1)[0][0]\n",
    "    return [dataMode for i in test]\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def cross_validation_data(dataset, folds):\n",
    "    dataCopy = dataset[:]\n",
    "    foldLen = len(dataCopy)//folds\n",
    "    crossData = []\n",
    "    for i in range(folds - 1):\n",
    "        currFold = []\n",
    "        for j in range(foldLen):\n",
    "            currData = random.choice(dataCopy)\n",
    "            currFold.append(currData)\n",
    "            dataCopy.pop(dataCopy.index(currData))\n",
    "        crossData.append(currFold)\n",
    "    currFold = []\n",
    "    for i in range(len(dataCopy)):\n",
    "            currData = random.choice(dataCopy)\n",
    "            currFold.append(currData)\n",
    "            dataCopy.pop(dataCopy.index(currData))\n",
    "    crossData.append(currFold)\n",
    "    return crossData\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, folds, metric, *args):\n",
    "    foldedData = cross_validation_data(dataset, folds)\n",
    "    scores = []\n",
    "    for i in range(len(foldedData)):\n",
    "        copyFolded = foldedData[:]\n",
    "        test_data = copyFolded.pop(i)\n",
    "        test = [test_data[j][:-1] for j in range(len(test_data))]\n",
    "        for j in test:\n",
    "            j.append(None)\n",
    "        train = []\n",
    "        for fold in copyFolded:\n",
    "            train += fold\n",
    "        predicted = algorithm(train,test, *args)\n",
    "        actual = [j[-1] for j in test_data]\n",
    "        result = metric(actual,predicted)\n",
    "        scores.append(result)\n",
    "    return scores\n",
    "\n",
    "\n",
    "filename = 'abalone.csv'\n",
    "abaloneData = load_data(filename)\n",
    "print('Number of Instances:', len(abaloneData), 'Number of Features:', len(abaloneData[0]))\n",
    "for column in range(1,len(abaloneData[0])):\n",
    "    column2Float(abaloneData, column)\n",
    "    \n",
    "def abaloneClass(data):\n",
    "    for i in data: \n",
    "        if i[0] == 'M':\n",
    "            i[0] = 1\n",
    "        elif i[0] == 'F':\n",
    "            i[0] = 0\n",
    "        elif i[0] == 'I':\n",
    "            i[0] = 2\n",
    "            \n",
    "abaloneClass(abaloneData)\n",
    "\n",
    "abaloneCopy = abaloneData[:]\n",
    "print('abaloneData first row', abaloneCopy[0])\n",
    "folds = 5\n",
    "k = 5\n",
    "scores = evaluate_algorithm(abaloneCopy, knn, folds, accuracy,k)\n",
    "zeroRCScores = evaluate_algorithm(abaloneCopy, zeroRC, folds, accuracy)\n",
    "print('knn:', scores)\n",
    "print('knn Min: %.3f' % min(scores), 'knn Max: %.3f' % max(scores), 'knn Mean: %.3f' % mean(scores))\n",
    "print('zeroRC: ', zeroRCScores)\n",
    "print('zeroRC Min: %.3f' % min(zeroRCScores), 'zeroRC Max: %.3f' % max(zeroRCScores), 'zeroRC Mean: %.3f' % mean(zeroRCScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write up results here\n",
    "Knn worked better than zeroR for classification of the abalone dataset but not by much KNN mean was 23% while zeroR mean 2as 16.5%. From the results I would not reccomend using KNN on this dataset and we should try another classification algorithm instead. We also do not know from this which attributes were dominant in classification and if by removing some attributes we may have better results, because all we know are which datapoints are closest to the training datapoints. From this we may be able to tell that the data did not have distinct clusters based on age alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: KNN regression\n",
    "\n",
    "We can also run KNN as a regression algorithm. In this case, instead of predicting the most common class in the k nearest neighbors, we can assign a predicted value that is the mean of the values in the k neighbors. \n",
    "\n",
    "Make this change to your algorithm (presumably by simply implementing a new predict function below, because you divided your code up sensibly in Part 1), and run the abalone data as a regression problem (convert the class data to a float, before running the algorithm). Use the same number of folds and the same k value as before. Also run a regression baseline and report RMSE values for both. Give me some explanation of the results, both standalone and in comparison to the classification results above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knnRegression: [2.169566307830174, 2.272512129174563, 2.1622786827318077, 2.1966114350078354, 2.379431977435985]\n",
      "knnRegression Min: 2.162 knnRegression Max: 2.379 knnRegression Mean: 2.236\n",
      "zeroRR:  [9.893305049173978, 9.897647465191824, 9.884595083099303, 10.045356035694127, 10.010862489546504]\n",
      "zeroRR Min: 9.885 zeroRR Max: 10.045 zeroRR Mean: 9.946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def knnRegression(train, test, k):\n",
    "    results = []\n",
    "    for i in range(len(test)):\n",
    "        testDistances = []\n",
    "        for j in range(len(train)):\n",
    "            testDistances.append(euclidDistance(test[i], train[j]))\n",
    "        kClosestList = kClosest(testDistances, k)\n",
    "        nearistNeighbors = [train[testDistances.index(i)] for i in kClosestList]\n",
    "        nearistClasses = [i[-1] for i in nearistNeighbors]\n",
    "        results.append(mean(nearistClasses))\n",
    "    return results\n",
    "\n",
    "def zeroRR(train, test):\n",
    "    trainY = [i[1] for i in train]\n",
    "    testY = [i[1] for i in test]\n",
    "\n",
    "    trainYMean = mean(trainY)\n",
    "    predictions = []\n",
    "    for i in testY:\n",
    "        predictions.append(trainYMean)\n",
    "    return predictions\n",
    "\n",
    "folds = 5\n",
    "k = 5\n",
    "scores = evaluate_algorithm(abaloneCopy, knnRegression, folds, rmse_eval,k)\n",
    "zeroRRScores = evaluate_algorithm(abaloneCopy, zeroRR, folds, rmse_eval)\n",
    "print('knnRegression:', scores)\n",
    "print('knnRegression Min: %.3f' % min(scores), 'knnRegression Max: %.3f' % max(scores), 'knnRegression Mean: %.3f' % mean(scores))\n",
    "print('zeroRR: ', zeroRRScores)\n",
    "print('zeroRR Min: %.3f' % min(zeroRRScores), 'zeroRR Max: %.3f' % max(zeroRRScores), 'zeroRR Mean: %.3f' % mean(zeroRRScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write up results here\n",
    "\n",
    "KNN regression worked much better then the base line of zeroR with an RMSE of 2.26 versus an RMSE of 10. Without knowing how RMSE relates to classification I cannot for sure say that KNN regression is better than KNN classification for this dataset however it outperformed its base line much more than KNN classification did. This may mean that the age of the abalone followed some linear pattern based on its attributes where clusters may overlap but older abalone had similar features and younger abalone had similar features. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
