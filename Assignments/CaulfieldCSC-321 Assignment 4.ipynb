{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-321: Data Mining and Machine Learning\n",
    "## Assignment 4: Multivariate Linear Regression\n",
    "\n",
    "\n",
    "For more complex learning, we cannot simply read our coefficients (b0 and b1 for SLR) from the data. We have to approximate them because there are many more of them. If you remember, the function for SLR was:\n",
    "\n",
    "y = b0 + b1*x\n",
    "\n",
    "For the more complicated Multivariate Linear Regression the function is:\n",
    "\n",
    "y = b0 + b1*x1 + b2*x2 + ... + bN*xN\n",
    "\n",
    "With there being as many coefficients (b) as there are input features, plus 1 (for the b0 coefficient, or intercept). \n",
    "\n",
    "We don't know exactly how they play together for a given data set so we're going to use a mechanism to pick some random values, and gradually improve them over time.\n",
    "\n",
    "The method we discussed in class is called Stochastic Gradient Descent, and is one of a variety of related optimization algorithms that are used in many machine learning methods to 'learn' the coefficients, or weights, on input values with respect to some output. \n",
    "\n",
    "Gradient descent it the process of minimizing some function (we'll call it the cost, or error function) by following the slope (or gradient) of the function down to some minimum. Intuitively, we're going to show our model one training instance at a time, make a prediction for that instance, calculate the error using that instance, and update the model in order to improve performance (get a smaller error) for the nect prediction. We'll repeat this process for a number of iterations, minimizing the error each time.\n",
    "\n",
    "Each iteration, we're going to update the coefficients using the formula:\n",
    "\n",
    "b = b - learning_rate * error * x\n",
    "\n",
    "for each coefficient (again, corresponding to each input feature, of each instance).\n",
    "\n",
    "Remember also that learning_rate is a value we must choose (I'll tell you to start with), and the number of iterations is ALSO a number we must choose. Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Making Predictions\n",
    "\n",
    "(a) We're going to use predictions made by our model as a guide for tuning the coefficients, we need a function that can make predictions. This function can also apply the final coefficients we have learned to make predictions over new data. Write a function called predict(instance, coefficients), that takes a single instance (a list of input features), and a list of coefficients, and calculates the predicted y value, using the formula:\n",
    "\n",
    "y = b0 + b1*x1 + b2*x2 + ... + bN*xN\n",
    "\n",
    "This should work for ANY length of instance, but we can always assume that the length of the instance list and the length of the coefficent list are the SAME. For the instance, the values are:\n",
    "\n",
    "[x1,x2,x3,...,xN,Y] (where Y is the actual value, in the case of training, or None, in the case of testing).\n",
    "\n",
    "For the coefficients, let's assume that the list contains all the coefficients, including b0. Let's also assume that coefficients[0] is ALWAYS where we store b0. \n",
    "\n",
    "Your function predict(instance, coefficients) should return the predicted y value for a given instance and set of coefficients. \n",
    "\n",
    "(b) In the Simple Linear Regression assignment, you applied your model to a contrived data set. I've reproduced this below. Go through this data set one instance at a time and call your new predict function for each instance. You can use the coefficients [0.4, 0.8], which are almost exactly what you learned as coefficients in Assignment 2. \n",
    "\n",
    "For each instance, print out the correct value, and the value predicted by your function from (a). You should see that it performs reasonably well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 1.2000000000000002 real: 1\n",
      "prediction: 2.0 real: 3\n",
      "prediction: 3.6 real: 3\n",
      "prediction: 2.8000000000000003 real: 2\n",
      "prediction: 4.4 real: 5\n"
     ]
    }
   ],
   "source": [
    "# Write your predict function here\n",
    "def predict(instance, coefficients):\n",
    "    y = coefficients[0]\n",
    "    for i in range(len(instance)-1):\n",
    "        y += instance[i]*coefficients[i+1]\n",
    "    return y\n",
    "\n",
    "# Apply your function to the contrived dataset\n",
    "\n",
    "dataset = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n",
    "coef = [0.4,0.8]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    print('prediction:', predict(dataset[i], coef), 'real:', dataset[i][1])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Learning coefficients with Stochastic Gradient Descent\n",
    "\n",
    "We now need to estimate coefficients for ourselves. For a given learning rate, for a number of iterations (epochs), we're going to estimate our coefficients.  Given a set of training data, we're going to:\n",
    "\n",
    "- loop over each epoch\n",
    "- loop over each row (instance) in the training data, for an epoch\n",
    "- loop over each coefficient, for each feature in each instanc, for each epoch\n",
    "\n",
    "As computer scientists you should recognize that this requires three, nested for loops, and you should have a sense (a big-O kind of sense) why this can take a long time for large data sets.\n",
    "\n",
    "Coefficients are updated based on the error the model made. The error is calculated as the difference between the predicted y value and the actual y value:\n",
    "\n",
    "error = prediction - actual\n",
    "\n",
    "There is one coefficient for EACH input attribute, and these are updated every time, for example:\n",
    "\n",
    "b1 = b1 - learning_rate * error * x1\n",
    "\n",
    "We ALSO need to update the special intercept coefficient b0:\n",
    "\n",
    "b0 = b0 - learning_rate * error\n",
    "\n",
    "(c) Implement the following algorithm for Stochastic Gradient Descent, naming your function coefficientsSGD(train, learning_rate, epochs), where train is the training data, and the other two parameters are the ones that control the learning.\n",
    "\n",
    "The algorithm is as follows: \n",
    "\n",
    "- initialize a list for the output coefficients. The length of the list will be the same as the length of every instance in the training data. We can initialize all the coefficients to 0.0 in the first instance\n",
    "- for each epoch\n",
    "    - initialize the total error to 0\n",
    "    - for each instance in the training data\n",
    "        - calculate the prediction for that instance, given the current list of coefficients, using our function from (a)\n",
    "        - calculate the error for that prediction \n",
    "        - (remember, each instance of the training data has the actual Y value)\n",
    "        - square the error, and add it to the total error. We're going to print the total error each time, and squaring the individual error means it will always be a positive value. NOTE: We don't use this squared error for updating the coefficients - we use the original error. This squaring is just to give us nice, readable output. \n",
    "        - Now update the coefficients, using the formulas given above. One update for b0 (which should always be at position 0 in the coefficients list), and then a series of updates for the remaining coefficients, b1 through bN.\n",
    "        \n",
    "    - At the end of each epoch, print out the epoch number (we can start at epoch 0), the learning rate, and the total error for this epoch.\n",
    "- once we've iterated through each epoch, return the list of coefficients\n",
    "\n",
    "(d) Apply your coefficientsSGD function to the contrived dataset, given below. If it's working, you should see the error rate falling each epoch. You should also note that the value of the coefficients learned isn't quite the same as Simple Linear Regression, because we're estimating each time. You could try learning longer (more epochs), or altering the learning rate, and see if the coefficients approach the optimal values we learned in Assignment 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 lrate= 0.001 error= 46.23569225016471\n",
      "epoch= 1 lrate= 0.001 error= 41.305142323835085\n",
      "epoch= 2 lrate= 0.001 error= 36.92968879875065\n",
      "epoch= 3 lrate= 0.001 error= 33.046843407651664\n",
      "epoch= 4 lrate= 0.001 error= 29.601151923716085\n",
      "epoch= 5 lrate= 0.001 error= 26.543402390484545\n",
      "epoch= 6 lrate= 0.001 error= 23.82992247422831\n",
      "epoch= 7 lrate= 0.001 error= 21.421955907121237\n",
      "epoch= 8 lrate= 0.001 error= 19.285109118735654\n",
      "epoch= 9 lrate= 0.001 error= 17.38886015544335\n",
      "epoch= 10 lrate= 0.001 error= 15.706122876572774\n",
      "epoch= 11 lrate= 0.001 error= 14.212860205347214\n",
      "epoch= 12 lrate= 0.001 error= 12.88774091297372\n",
      "epoch= 13 lrate= 0.001 error= 11.7118350357667\n",
      "epoch= 14 lrate= 0.001 error= 10.668343576747102\n",
      "epoch= 15 lrate= 0.001 error= 9.742358632631682\n",
      "epoch= 16 lrate= 0.001 error= 8.920650521505976\n",
      "epoch= 17 lrate= 0.001 error= 8.191478871959525\n",
      "epoch= 18 lrate= 0.001 error= 7.544424976557279\n",
      "epoch= 19 lrate= 0.001 error= 6.970243016110007\n",
      "epoch= 20 lrate= 0.001 error= 6.4607280306236845\n",
      "epoch= 21 lrate= 0.001 error= 6.00859875189941\n",
      "epoch= 22 lrate= 0.001 error= 5.607393624934783\n",
      "epoch= 23 lrate= 0.001 error= 5.251378533574368\n",
      "epoch= 24 lrate= 0.001 error= 4.935464912958705\n",
      "epoch= 25 lrate= 0.001 error= 4.6551370796144145\n",
      "epoch= 26 lrate= 0.001 error= 4.40638774162885\n",
      "epoch= 27 lrate= 0.001 error= 4.185660768141068\n",
      "epoch= 28 lrate= 0.001 error= 3.9898004010231354\n",
      "epoch= 29 lrate= 0.001 error= 3.8160061836022887\n",
      "epoch= 30 lrate= 0.001 error= 3.6617929628979957\n",
      "epoch= 31 lrate= 0.001 error= 3.5249553942839214\n",
      "epoch= 32 lrate= 0.001 error= 3.4035364417673994\n",
      "epoch= 33 lrate= 0.001 error= 3.295799424125944\n",
      "epoch= 34 lrate= 0.001 error= 3.2002032077659255\n",
      "epoch= 35 lrate= 0.001 error= 3.1153801920958983\n",
      "epoch= 36 lrate= 0.001 error= 3.0401167730772007\n",
      "epoch= 37 lrate= 0.001 error= 2.9733360059968246\n",
      "epoch= 38 lrate= 0.001 error= 2.914082219907388\n",
      "epoch= 39 lrate= 0.001 error= 2.8615073640442605\n",
      "epoch= 40 lrate= 0.001 error= 2.8148588912588495\n",
      "epoch= 41 lrate= 0.001 error= 2.7734690054522004\n",
      "epoch= 42 lrate= 0.001 error= 2.736745119468375\n",
      "epoch= 43 lrate= 0.001 error= 2.7041613871898917\n",
      "epoch= 44 lrate= 0.001 error= 2.6752511889152393\n",
      "epoch= 45 lrate= 0.001 error= 2.649600462709536\n",
      "epoch= 46 lrate= 0.001 error= 2.6268417864985882\n",
      "epoch= 47 lrate= 0.001 error= 2.606649126395945\n",
      "epoch= 48 lrate= 0.001 error= 2.5887331762654906\n",
      "epoch= 49 lrate= 0.001 error= 2.572837221964086\n",
      "Coefficients are: [0.22998234937311363, 0.8017220304137576]\n"
     ]
    }
   ],
   "source": [
    "# Write your coefficientsSGD(train,learning_rate,epochs) here\n",
    "\n",
    "def coefficientsSGD(train, learning_rate, epochs):\n",
    "    coefficients = [0 for i in range(len(train[0]))]\n",
    "    for e in range(epochs):\n",
    "        totalError = 0\n",
    "        for instance in train:\n",
    "            predY = predict(instance, coefficients)\n",
    "            error = predY - instance[-1]\n",
    "            totalError += error**2\n",
    "            coefficients[0] -= learning_rate*error\n",
    "            for i in range(1,len(coefficients)):\n",
    "                coefficients[i] -= learning_rate*error*instance[i-1]\n",
    "        print('epoch=', e, 'lrate=', learning_rate, 'error=', totalError)\n",
    "    return coefficients\n",
    "                \n",
    "# Apply to the contrived data here. Try my parameters first, before you experiment\n",
    "\n",
    "dataset = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "coefs = coefficientsSGD(dataset, learning_rate, epochs)\n",
    "print('Coefficients are:',coefs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Now you have sufficient functionality to write a function to make predictions using multivariate linear regression. Create a function with the signature mlr(train,test,learning_rate,epochs). Remember, training data is data containing the features of the data AND the class. Testing data contains the features, but does NOT contain a class (instead it should hold the value None in place of the class entry). \n",
    "\n",
    "We're going to use the same dataset here for both training and testing, even though we know that might not be a great idea.\n",
    "\n",
    "Here's the mlr algorithm. We're going to estimate our coefficients from the training data, using the function from (c) above. We're going to create a new list, to hold our predictions. Then for each entry in the testing data, we're going to read the input value, and make a prediction, using our function from (a). For each entry in the test data, we're going to append our predicted y value to the prediction list. We're going to return our list of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function mlr(train,test,learning_rate,epochs) here\n",
    "\n",
    "def mlr(train, test, learning_rate, epochs):\n",
    "    coefficients = coefficientsSGD(train, learning_rate, epochs)\n",
    "    predictions = []\n",
    "    for entry in test:\n",
    "        prediction = predict(entry, coefficients)\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Applying to real data\n",
    "\n",
    "Last time I gave you the wine quality data set. Let's use that for this assignment. You'll need to:\n",
    "\n",
    "(f) Load the data set\n",
    "(g) Convert the features from strings to floats\n",
    "(h) Normalize all the attributes\n",
    "(i) Call the evaluate_algorithm function, given below with both your mlr algorithm, and a baseline algorithm (zeroRR). Print out the RMSE for both. \n",
    "\n",
    "Executing the above will require you to copy across a number of functions from the previous assignments.\n",
    "At the end, write something about the result. To do that, it will be helpful to interpret both the final RMSE from the multivariate linear regression, and to know something about the original features. The wine data set is just the white wine component of the data you can find here: \n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 lrate= 0.01 error= 94.49427503765666\n",
      "epoch= 1 lrate= 0.01 error= 80.86482419578945\n",
      "epoch= 2 lrate= 0.01 error= 78.74976548229141\n",
      "epoch= 3 lrate= 0.01 error= 77.65986226752669\n",
      "epoch= 4 lrate= 0.01 error= 77.03161826055162\n",
      "epoch= 5 lrate= 0.01 error= 76.64462946945235\n",
      "epoch= 6 lrate= 0.01 error= 76.39423515123764\n",
      "epoch= 7 lrate= 0.01 error= 76.22594572954972\n",
      "epoch= 8 lrate= 0.01 error= 76.10930852419794\n",
      "epoch= 9 lrate= 0.01 error= 76.02627901751154\n",
      "epoch= 10 lrate= 0.01 error= 75.96565474337895\n",
      "epoch= 11 lrate= 0.01 error= 75.92023083082891\n",
      "epoch= 12 lrate= 0.01 error= 75.88525248856908\n",
      "epoch= 13 lrate= 0.01 error= 75.85752342887567\n",
      "epoch= 14 lrate= 0.01 error= 75.8348665047151\n",
      "epoch= 15 lrate= 0.01 error= 75.81578448152017\n",
      "epoch= 16 lrate= 0.01 error= 75.79924030111283\n",
      "epoch= 17 lrate= 0.01 error= 75.78451162948105\n",
      "epoch= 18 lrate= 0.01 error= 75.77109304880543\n",
      "epoch= 19 lrate= 0.01 error= 75.75862952887735\n",
      "epoch= 20 lrate= 0.01 error= 75.74687078919008\n",
      "epoch= 21 lrate= 0.01 error= 75.735639790114\n",
      "epoch= 22 lrate= 0.01 error= 75.72481086971197\n",
      "epoch= 23 lrate= 0.01 error= 75.71429451214438\n",
      "epoch= 24 lrate= 0.01 error= 75.70402670046782\n",
      "epoch= 25 lrate= 0.01 error= 75.69396145248837\n",
      "epoch= 26 lrate= 0.01 error= 75.68406557460304\n",
      "epoch= 27 lrate= 0.01 error= 75.67431496584015\n",
      "epoch= 28 lrate= 0.01 error= 75.66469200819586\n",
      "epoch= 29 lrate= 0.01 error= 75.65518371996443\n",
      "epoch= 30 lrate= 0.01 error= 75.64578044610167\n",
      "epoch= 31 lrate= 0.01 error= 75.63647492734782\n",
      "epoch= 32 lrate= 0.01 error= 75.62726163696539\n",
      "epoch= 33 lrate= 0.01 error= 75.61813630693113\n",
      "epoch= 34 lrate= 0.01 error= 75.60909558849663\n",
      "epoch= 35 lrate= 0.01 error= 75.60013680822921\n",
      "epoch= 36 lrate= 0.01 error= 75.59125779205472\n",
      "epoch= 37 lrate= 0.01 error= 75.58245673782054\n",
      "epoch= 38 lrate= 0.01 error= 75.57373212258813\n",
      "epoch= 39 lrate= 0.01 error= 75.56508263483373\n",
      "epoch= 40 lrate= 0.01 error= 75.55650712458326\n",
      "epoch= 41 lrate= 0.01 error= 75.5480045664981\n",
      "epoch= 42 lrate= 0.01 error= 75.53957403235756\n",
      "epoch= 43 lrate= 0.01 error= 75.53121467038974\n",
      "epoch= 44 lrate= 0.01 error= 75.52292568961744\n",
      "epoch= 45 lrate= 0.01 error= 75.51470634790282\n",
      "epoch= 46 lrate= 0.01 error= 75.50655594273714\n",
      "epoch= 47 lrate= 0.01 error= 75.49847380408058\n",
      "epoch= 48 lrate= 0.01 error= 75.49045928874604\n",
      "epoch= 49 lrate= 0.01 error= 75.48251177596279\n",
      "MLR RMSE: 0.127\n",
      "zeroRR RMSE: 0.148\n"
     ]
    }
   ],
   "source": [
    "# Write your code for f through i here\n",
    "import csv\n",
    "\n",
    "# Load the data\n",
    "def load_data(filename):\n",
    "    csvTxt = csv.reader(open(filename))\n",
    "    data = []\n",
    "    for row in csvTxt:\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "def column2Float(dataset,column):\n",
    "    for instance in dataset:\n",
    "        instance[column] = float(instance[column])\n",
    "    return dataset\n",
    "\n",
    "import math\n",
    "def mean(listOfValues):\n",
    "    total = 0\n",
    "    for num in listOfValues:\n",
    "        total += num\n",
    "    return total/len(listOfValues)\n",
    "\n",
    "def zeroRR(train, test):\n",
    "    trainY = [i[-1] for i in train]\n",
    "    testY = [i[-1] for i in test]\n",
    "\n",
    "    trainYMean = mean(trainY)\n",
    "    predictions = []\n",
    "    for i in testY:\n",
    "        predictions.append(trainYMean)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def rmse_eval(actual, predicted):\n",
    "    error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        error += (predicted[i] - actual[i])**2\n",
    "    error = error/len(actual)\n",
    "    error = error**0.5\n",
    "    return error\n",
    "\n",
    "def minmax(dataset):\n",
    "    listMinMax = []\n",
    "    for column in range(len(dataset[0])):\n",
    "        columnData = [dataset[i][column] for i in range(len(dataset))]\n",
    "        listMinMax.append([min(columnData), max(columnData)])\n",
    "    return listMinMax\n",
    "\n",
    "def normalize(dataset, minmax):\n",
    "    for row in range(len(dataset)):\n",
    "        for column in range(len(dataset[row])):\n",
    "            dataset[row][column] = (dataset[row][column] - minmax[column][0]) / (minmax[column][1] - minmax[column][0])\n",
    "\n",
    "wineData = load_data('winequality-white.csv')\n",
    "#print('loaded wine data', wineData[0])\n",
    "for column in range(len(wineData[0])):\n",
    "    column2Float(wineData, column)\n",
    "#print('wine data as floats', wineData[0])\n",
    "minmaxWine = minmax(wineData)\n",
    "normalize(wineData, minmaxWine)\n",
    "#print('wine data normalized', wineData[0])\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, metric, *args):\n",
    "    train = dataset\n",
    "    test = [dataset[i][:-1] for i in range(len(dataset))]\n",
    "    for i in test:\n",
    "        i.append(None)\n",
    "    predicted = algorithm(train,test,*args)\n",
    "    actual = [i[-1] for i in dataset]\n",
    "    result = metric(actual,predicted)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Testing multivariate linear regression\n",
    "\n",
    "dataset = wineData\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "mlr_result = evaluate_algorithm(dataset,mlr,rmse_eval,learning_rate,epochs)\n",
    "zeroRR_result = evaluate_algorithm(dataset,zeroRR,rmse_eval)\n",
    "\n",
    "print('MLR RMSE: %.3f' % mlr_result)\n",
    "print('zeroRR RMSE: %.3f' % zeroRR_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2 Discussion Here\n",
    "\n",
    "The MLR found wine quality 0.03 closer to the actual wine quality than zeroRR using RMSE as the evaluation metric. The wine quality is on a scale of 0 to 10 so an improvement of 0.03 does not seem that great for the effort involved. I briefly looked over the data and most of the wine quality was either 5, 6, or 7 which would be around the mean. This makes sense as to why the zeroR worked so well because most of the ouput data was near the mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
