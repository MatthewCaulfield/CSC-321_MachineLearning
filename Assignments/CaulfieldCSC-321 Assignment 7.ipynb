{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-321: Data Mining and Machine Learning\n",
    "# Matthew Caulfield\n",
    "## Assignment 7: Classification with probability\n",
    "\n",
    "### Part 1: Naive Bayes\n",
    "\n",
    "Everything so far has been a linear classifier. Now we'll move up a gear, and implement some non-linear classifiers. The first, as we saw in class, is Naive Bayes, that makes use of proability to make predictions.\n",
    "\n",
    "We make use of Bayes Theorem, that allows us to calculate the probability of a piece of data belonging to a given class, given our prior knowledge. Bayes Theorem is stated as:\n",
    "\n",
    "P(class|data) = (P(data|class) * P(class)) / P(data)\n",
    "\n",
    "Where P(class|data) is the probability of class given the provided data\n",
    "\n",
    "We're going to break this down into several steps. Again, I've given you a contrived data set for you to test your functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Separate by class\n",
    "\n",
    "Just as in class, we need to calculate the probability of data by the class they belong to. We'll need to separate our data by the class. Create a dictionary, where the key is class, and the values is a list of all instances with that class value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data: [[3.393533211, 2.331273381], [3.110073483, 1.781539638], [1.343808831, 3.368360954], [3.582294042, 4.67917911], [2.280362439, 2.866990263]]\n",
      "1 data: [[7.423436942, 4.696522875], [5.745051997, 3.533989803], [9.172168622, 2.511101045], [7.792783481, 3.424088941], [7.939820817, 0.791637231]]\n"
     ]
    }
   ],
   "source": [
    "# Contrived data set\n",
    "import statistics\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "dataset = [[3.393533211,2.331273381,0],\n",
    "    [3.110073483,1.781539638,0],\n",
    "    [1.343808831,3.368360954,0],\n",
    "    [3.582294042,4.67917911,0],\n",
    "    [2.280362439,2.866990263,0],\n",
    "    [7.423436942,4.696522875,1],\n",
    "    [5.745051997,3.533989803,1],\n",
    "    [9.172168622,2.511101045,1],\n",
    "    [7.792783481,3.424088941,1],\n",
    "    [7.939820817,0.791637231,1]]\n",
    "\n",
    "\n",
    "# implement separateByClass(dataset) here\n",
    "\n",
    "def seperateByClass(dataset):\n",
    "    classDict = {}\n",
    "    for dataPoint in dataset:\n",
    "        currClass = dataPoint[-1]\n",
    "        if currClass not in classDict:\n",
    "            classDict[currClass] = [dataPoint[:-1]]\n",
    "        else:\n",
    "            classDict[currClass].append(dataPoint[:-1])\n",
    "    return classDict\n",
    "testDict = seperateByClass(dataset)\n",
    "print('0 data:', testDict[0])\n",
    "print('1 data:', testDict[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Summarize the data\n",
    "\n",
    "We need two statistics from the data, the mean and the standard deviation. You should have these functions in a previous assignment, remembering the standard deviation is simply the square root of the variance. We need the mean and standard deviation for each of our attributes, i.e. for each column of our data. Create a function that summarizes a given data set, by gathering all of the information for each column, and calculating the mean and standard deviation on that columns data. We'll collect this information into a tuple, one per column, comprising the mean, the standard deviation and the number of elements in each column). Return a list of these tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10), (0.5, 0.5270462766947299, 10)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# implement summarizeDataset(dataset) here, and copy across any functions you need to help you\n",
    "\n",
    "def mean(listOfValues):\n",
    "    total = 0\n",
    "    for num in listOfValues:\n",
    "        total += num\n",
    "    return total/len(listOfValues)\n",
    "\n",
    "def variance(listOfValues, meanValue):\n",
    "    total = 0\n",
    "    for num in listOfValues:\n",
    "       total +=  (num - meanValue)**2/(len(listOfValues)-1)\n",
    "    return total\n",
    "\n",
    "def summarizeDataset(dataset):\n",
    "    summaryData = []\n",
    "    for col in range(len(dataset[0])):\n",
    "        currCol = []\n",
    "        for dataPoint in dataset:\n",
    "            currCol.append(dataPoint[col])\n",
    "        colMean = mean(currCol)\n",
    "        colVar = variance(currCol, colMean)\n",
    "        colStDev = colVar**0.5\n",
    "        summaryData.append((colMean, colStDev, len(currCol)))\n",
    "    return summaryData\n",
    "\n",
    "testSummary = summarizeDataset(dataset)\n",
    "print(testSummary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Summarize data by class\n",
    "\n",
    "We now need to combine the functions from (a) and (b) above. Create a summarizeByClass function, that splits the data by class, and then caluclates statistics for each row of the data for each class. The results - the list of tuples of statistics, one per column - should then be stored in a dictionary by their class value. summarizeByClass should return such a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(2.7420144012, 0.9265683289298018, 5), (3.0054686692, 1.1073295894898725, 5)], 1: [(7.6146523718, 1.2344321550313704, 5), (2.9914679790000003, 1.4541931384601618, 5)]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# implement summarizeByClass(dataset) here\n",
    "def summarizeByClass(dataset):\n",
    "    classDict = seperateByClass(dataset)\n",
    "    summaryDict = {}\n",
    "    for currClass in classDict:\n",
    "        classData = classDict[currClass]\n",
    "        classSummary = summarizeDataset(classData)\n",
    "        summaryDict[currClass] = classSummary\n",
    "    return summaryDict\n",
    "\n",
    "\n",
    "# The dictionary for the contrived data should look like:\n",
    "# {0: [(2.7420144012, 0.9265683289298018, 5), (3.0054686692, 1.1073295894898725, 5)], 1: [(7.6146523718, 1.2344321550313704, 5), (2.9914679790000003, 1.4541931384601618, 5)]}\n",
    "\n",
    "testSummaryClass = summarizeByClass(dataset)\n",
    "print(testSummaryClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Guassiaun Probability Density\n",
    "\n",
    "We're working with numerical data here, so we need to implement the gaussian probability density function (PDF) we talked about in class, so we can attach probabilities to real values. A gaussian distribution can be summarized from two values - guess which two? If you guessed mean and standard deviation, you were correct. The gaussian PDF is calculated as follows:\n",
    "\n",
    "probability(x) = (1 / (sqrt(2 * pi) * std_dev)) * exp(-((x-mean) ** 2 / 2 * std_dev ** 2 )))\n",
    "\n",
    "Hopefully, you can see why we're going to need the mean and the std_dev from function (c)\n",
    "\n",
    "Create a function that:\n",
    "- takes a value\n",
    "- takes a mean\n",
    "- takes a standard deviation\n",
    "\n",
    "and returns the probability of seeing that value, given that distribution, using the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24197072451914337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Implement calcProb(value, mean, std_dev) here\n",
    "def calcProb(value, mean, std_dev):\n",
    "    probability = ((1 / math.sqrt(2 * math.pi * std_dev)) * math.exp(-((value-mean) ** 2) / (2 * std_dev ** 2 )))\n",
    "    return probability\n",
    "\n",
    "print(calcProb(3, 2, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Class Probabilities\n",
    "\n",
    "We can now use probabilites calculated from our training data to calculate probabilities for an instance of new data, by creating a function called calcClassProbs. Probabilites have to be calculated separately for each possible class in our data, so for each class we have to calculate the likelihood the new instance of data belongs to that class. The probability that a piece of data belongs to a class is calculated by:\n",
    "\n",
    "p(class|data) = p(X|class) * P(class)\n",
    "\n",
    "The divison has been removed, because we're just trying to maximize the result of the formula above. The largest value we get for each class above determines which class we assign. Each input value is treated separately, so in the case where we have TWO input values in our data (X1 and X2), the probablility that an instance belongs to class 0 is calculated by:\n",
    "\n",
    "P(class=0|X1,X2) = P(X1|class=0) * P(X2|class=0) * P(class=0)\n",
    "\n",
    "We have to repeat this for each class, and then choose the class with the highest score. We should not assume a fixed number of input features, X, the above was just an illustration. \n",
    "\n",
    "We'll start by creating a function that will return the probabilities of predicting each class for a given instance. This function will take a dictionary of summaries (as returned by (c), above) and an instance, and will generate a dictionary of probabilites, with one entry per class. The steps are as follows:\n",
    "\n",
    "- We need to calculate the total number of training instances, by counting the counts stored in the summary statistics. So if there are 9 instances with one label, and 5 with another (as in the weather data) then we need to know there are 14 instances. \n",
    "\n",
    "- This will help us calculate the probability of a given class, the prior probability P(class), as the ratio of rows with a given class divided by all rows in the training data\n",
    "\n",
    "- Next probabilities are calculated for each input value in the instance, using the gaussian PDF, and the statistics for that column and of that class. Probabilites are multiplied together as they are accumulated with the formula given above. \n",
    "\n",
    "- The process is repeated for each class in the data\n",
    "\n",
    "- Return the dictionary of probabilities for each class for the new instance\n",
    "\n",
    "Some things that might help with implementation. \n",
    "\n",
    "- Dictionaries are your friend here\n",
    "- The data returned by (c) above is already divided by class. You can:\n",
    "    - discover the prior probability from this data (how many instances for this class, divided by the total instances)\n",
    "    - iterate over the tuples, which give you the information (mean, std_dev, count) on a per column basis\n",
    "    - calculate probability given the attribute value corresponding to that column using your function from (d)\n",
    "\n",
    "Try this out on the contrived data. \n",
    "\n",
    "NOTE: If you want to output ACTUAL probabilities by class, we divide each score in the dictionary for an instance, by the sum of the values. You don't need to do this, it's just a reminder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities are: {0: 0.050974704886547935, 1: 0.00015485198134648}\n",
      "The probability of the instance belonging to class 0 is 99.70\n",
      "The probability of the instance belonging to class 1 is 0.30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Implement calcClassProbs(summaries, instance) here\n",
    "\n",
    "def calcClassProbs(summaries, instance):\n",
    "    probDict = {}\n",
    "    totalLen = 0\n",
    "    for currClass in summaries:\n",
    "        totalLen += summaries[currClass][0][-1]\n",
    "    for currClass in summaries:\n",
    "        priorProb = summaries[currClass][0][-1]/totalLen\n",
    "        #print(priorProb)\n",
    "        classProb = priorProb\n",
    "        for i in range(len(summaries[currClass])):\n",
    "            colMean = summaries[currClass][i][0]\n",
    "            colStdDev = summaries[currClass][i][1]\n",
    "            #print('mean', colMean, 'stdDev', colStdDev)\n",
    "            colProb = calcProb(instance[i],colMean,colStdDev)\n",
    "            classProb *= colProb\n",
    "        probDict[currClass] = classProb\n",
    "    return probDict\n",
    "            \n",
    "\n",
    "# Test it out here\n",
    "\n",
    "summaries = summarizeByClass(dataset)\n",
    "probabilities = calcClassProbs(summaries, dataset[0])\n",
    "print('Probabilities are:',probabilities)\n",
    "\n",
    "# I think if everything works, it should be:\n",
    "# {0: 0.05032427673372075, 1: 0.00011557718379945765}\n",
    "# which according to the percentage calculation give above should be:\n",
    "# 99.77% in favour of class 0 \n",
    "\n",
    "sumProbs = sum([v for _,v in probabilities.items()])\n",
    "for k,v in probabilities.items():\n",
    "    print('The probability of the instance belonging to class %d is %.2f' % (k,v/sumProbs*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) Tying it all together\n",
    "\n",
    "You need to create a predict function. This function works very much as the example above, in that it takes a dictionary of summaries and a single row, and uses calcClassProbabilites to get the dictionary of probabilities. From this dictionary, find the largest value and corresponding class. Return this class. \n",
    "\n",
    "You also need a naiveBayes function, that takes a training set and a test set. It needs to generate summary statistics from the training set (using (c), above), then make predictions for each instance in the test set, by calling your predict function above for each instance, using the summaries generated. Append these predictions to a list you return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement predict(summaries,instance) here\n",
    "import operator\n",
    "def predict(summaries, instance):\n",
    "    probDict = calcClassProbs(summaries, instance)\n",
    "    return max(probDict.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Implement naiveBayes(train,test) here\n",
    "\n",
    "def naiveBayes(train, test):\n",
    "    classSummary = summarizeByClass(train)\n",
    "    return[predict(classSummary, instance) for instance in test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying to real data\n",
    "\n",
    "You've seen bits of the iris dataset in class. It's one of the most well known data sets in machine learning and data mining. So you might as well have a go at it! You can find out more about it here: http://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "You'll need to:\n",
    "\n",
    "- Load the data\n",
    "- convert all but the last column to floats\n",
    "- convert the last column to an int. There are THREE classes, so convert them to 0, 1 and 2 accordingly\n",
    "- call evaluate algorithm, using a 5-fold cross-validation\n",
    "- print the mean, min and max scores\n",
    "- compare this to some reasonable baseline\n",
    "- give me a very short write up of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances: 150 Number of Features: 5\n",
      "irisData first row [5.1, 3.5, 1.4, 0.2, 1]\n",
      "Bayes: [96.66666666666667, 96.66666666666667, 100.0, 93.33333333333333, 93.33333333333333]\n",
      "Bayes Min: 93.333 Bayes Max: 100.000 Bayes Mean: 96.000\n",
      "zeroRC:  [13.333333333333334, 30.0, 30.0, 20.0, 33.333333333333336]\n",
      "zeroRC Min: 13.333 zeroRC Max: 33.333 zeroRC Mean: 25.333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_data(filename):\n",
    "    csvTxt = csv.reader(open(filename))\n",
    "    data = []\n",
    "    for row in csvTxt:\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "def column2Float(dataset,column):\n",
    "    for instance in dataset:\n",
    "        instance[column] = float(instance[column])\n",
    "    return dataset\n",
    "\n",
    "def rmse_eval(actual, predicted):\n",
    "    error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        error += (predicted[i] - actual[i])**2\n",
    "    error = error/len(actual)\n",
    "    error = error**0.5\n",
    "    return error\n",
    "\n",
    "def minmax(dataset):\n",
    "    listMinMax = []\n",
    "    for column in range(len(dataset[0])):\n",
    "        columnData = [dataset[i][column] for i in range(len(dataset))]\n",
    "        listMinMax.append([min(columnData), max(columnData)])\n",
    "    return listMinMax\n",
    "\n",
    "def normalize(dataset, minmax):\n",
    "    for row in range(len(dataset)):\n",
    "        for column in range(len(dataset[row])):\n",
    "            dataset[row][column] = (dataset[row][column] - minmax[column][0]) / (minmax[column][1] - minmax[column][0])\n",
    "\n",
    "def accuracy(actual, predicted):\n",
    "    counter = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            counter += 1\n",
    "    return counter*100/len(actual)\n",
    "\n",
    "def zeroRC(train, test):\n",
    "    trainY = [i[-1] for i in train]\n",
    "    count = Counter(trainY)\n",
    "    dataMode = count.most_common(1)[0][0]\n",
    "    return [dataMode for i in test]\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def cross_validation_data(dataset, folds):\n",
    "    dataCopy = dataset[:]\n",
    "    foldLen = len(dataCopy)//folds\n",
    "    crossData = []\n",
    "    for i in range(folds - 1):\n",
    "        currFold = []\n",
    "        for j in range(foldLen):\n",
    "            currData = random.choice(dataCopy)\n",
    "            currFold.append(currData)\n",
    "            dataCopy.pop(dataCopy.index(currData))\n",
    "        crossData.append(currFold)\n",
    "    currFold = []\n",
    "    for i in range(len(dataCopy)):\n",
    "            currData = random.choice(dataCopy)\n",
    "            currFold.append(currData)\n",
    "            dataCopy.pop(dataCopy.index(currData))\n",
    "    crossData.append(currFold)\n",
    "    return crossData\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, folds, metric, *args):\n",
    "    foldedData = cross_validation_data(dataset, folds)\n",
    "    scores = []\n",
    "    for i in range(len(foldedData)):\n",
    "        copyFolded = foldedData[:]\n",
    "        test_data = copyFolded.pop(i)\n",
    "        test = [test_data[j][:-1] for j in range(len(test_data))]\n",
    "        for j in test:\n",
    "            j.append(None)\n",
    "        train = []\n",
    "        for fold in copyFolded:\n",
    "            train += fold\n",
    "        predicted = algorithm(train,test, *args)\n",
    "        actual = [j[-1] for j in test_data]\n",
    "        result = metric(actual,predicted)\n",
    "        scores.append(result)\n",
    "    return scores\n",
    "\n",
    "\n",
    "filename = 'iris.csv'\n",
    "irisData = load_data(filename)\n",
    "print('Number of Instances:', len(irisData), 'Number of Features:', len(irisData[0]))\n",
    "for column in range(len(irisData[0])-1):\n",
    "    column2Float(irisData, column)\n",
    "    \n",
    "def irisClass(data):\n",
    "    for i in data: \n",
    "        if i[-1] == 'Iris-setosa':\n",
    "            i[-1] = 1\n",
    "        elif i[-1] == 'Iris-virginica':\n",
    "            i[-1] = 0\n",
    "        elif i[-1] == 'Iris-versicolor':\n",
    "            i[-1] = 2\n",
    "            \n",
    "irisClass(irisData)\n",
    "\n",
    "irisCopy = irisData[:]\n",
    "print('irisData first row', irisCopy[0])\n",
    "folds = 5\n",
    "\n",
    "scores = evaluate_algorithm(irisCopy, naiveBayes, folds, accuracy)\n",
    "zeroRCScores = evaluate_algorithm(irisCopy, zeroRC, folds, accuracy)\n",
    "print('Bayes:', scores)\n",
    "print('Bayes Min: %.3f' % min(scores), 'Bayes Max: %.3f' % max(scores), 'Bayes Mean: %.3f' % mean(scores))\n",
    "print('zeroRC: ', zeroRCScores)\n",
    "print('zeroRC Min: %.3f' % min(zeroRCScores), 'zeroRC Max: %.3f' % max(zeroRCScores), 'zeroRC Mean: %.3f' % mean(zeroRCScores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write up your observations on the experiment here\n",
    "\n",
    "The baseline I used was zeroRC because it is a good based line for categorization. The baseline was lower than expected because I thought it should be correctly predict the answer more than 1/3 of the time when it only worked correctly 25% of the time. This could be due to how the folds are split. Naive Bayes was a good classifier and was able to classif the data with a mean score of 96%. This shows that the data had clear splits for certain attributes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
